{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDW73j_XTSgj"
      },
      "source": [
        "# Problem Set 5 - Reinforcement Learning and Transformers \n",
        "\n",
        "<h4> Reinforcement Learning Section by Sid Mysore. <br> Transformers Section adapted from previous homework designed by Ruizhao Zhu with help of Brian Kulis and Ashok Cutkosky<br> </h4>\n",
        "\n",
        "---\n",
        "\n",
        "This assignment is broken up into 2 parts:\n",
        "1. Implementing and testing basic deep RL algorithms for:\n",
        "    * Discrete action spaces with DQN (simplified from the seminal Nature paper)\n",
        "    * Continuous action spaces with DDPG (a popular basic actor-critic algorithm)\n",
        "2. Understanding the basic structure of Transformers\n",
        "\n",
        "This code has been tested locally on Linux, Windows 10 and MacOS, and on Colab\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: `Zhenghang Yin`\n",
        "\n",
        "BUID: `U82871437`"
      ],
      "metadata": {
        "id": "IfILmqacsKit"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhvhfW7VTSgz"
      },
      "source": [
        "## Notes before running code\n",
        "\n",
        "1. If running on Windows, note that some packages may throw warnings or errors - if you encounter this, try the fixes recommended by the error message(s)\n",
        "\n",
        "2. Additional packages may be required for running locally. This notebook is configured to help you install specific versions of the required packages but if you are concerned about them overwriting exisiting configs for other projects, it is recommended to create a new python environment.\n",
        "\n",
        "2. When running code, make sure the following files are in your current working directory as they will be needed for some of the RL logging and plotting:\n",
        "    * `test_policy.py`\n",
        "    * `logx.py`\n",
        "    * `plot.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDFtYqCYUY_q",
        "outputId": "f15d5c6b-160b-40f5-8c1a-6cc8dcd5eec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No gpus are found in this Notebook.\n"
          ]
        }
      ],
      "source": [
        "# Prerequisites\n",
        "\n",
        "# load all the files\n",
        "!ln -s /content/drive/MyDrive/Common\\ Data/CS523/hw5/logx.py logx.py\n",
        "!ln -s /content/drive/MyDrive/Common\\ Data/CS523/hw5/plot.py plot.py\n",
        "!ln -s /content/drive/MyDrive/Common\\ Data/CS523/hw5/test_policy.py test_policy.py\n",
        "\n",
        "\n",
        "def DPATH(*args: str) -> str:\n",
        "    from os.path import join\n",
        "    return join(\"/content/drive/MyDrive/Users/青椒/CS523/hw5_stuffs\", *args)\n",
        "\n",
        "\n",
        "def check_gpu() -> None:\n",
        "    result = !nvidia-smi --query-gpu=name --format=csv,noheader\n",
        "    answer = result[0].strip()\n",
        "    if len(answer) == 0 or \"nvidia-smi\" in (answer.lower()):\n",
        "        print(\"No gpus are found in this Notebook.\")\n",
        "    else:\n",
        "        print(f\"You are using: {answer}\")\n",
        "\n",
        "check_gpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puLqn0AgTSg1"
      },
      "source": [
        "## Q1 Deep Q Learning (30 Points)\n",
        "\n",
        "#### Key Concepts\n",
        "\n",
        "<img src=\"http://ai.bu.edu/DL523/HW5_files/rl_diagram.png\" width=\"360em\">\n",
        "\n",
        "The main components of the RL optimization loop are the agent and the environment. The environment is the world that the agent interacts with. At every step of interaction, the agent sees a (possibly partial) observation of the state of the world, and then decides on an action to take. The environment may change as a response to the agents' actions on it, but may also change on its own.\n",
        "\n",
        "The agent also perceives a reward signal from the environment, a number that tells it how good or bad the current world state is. The goal of the agent is to maximize its cumulative reward. Reinforcement learning methods are ways that the agent can learn behaviors to achieve its goal.\n",
        "\n",
        "##### Fully vs. Partially Observable State-spaces\n",
        "\n",
        "When the agent is able to observe the complete state of the environment, we say that the environment is fully observed. When the agent can only see a partial observation, we say that the environment is partially observed. For the purposes of this homework, we will be dealing with fully observable environments.\n",
        "\n",
        "##### Discrete vs. Continuous Action-spaces\n",
        "\n",
        "Different environments allow different kinds of actions. The set of all valid actions in a given environment is often called the action space. Some environments, like old Atari games and Go, have discrete action spaces, where only a finite number of moves are available to the agent. Other environments, like where the agent controls a robot in a physical world, have continuous action spaces. In continuous spaces, actions are real-valued vectors. In this homework, we will consider problems involving both types of action spaces and some simple algorithms for solving them.\n",
        "\n",
        "##### The RL Problem\n",
        "\n",
        "The reward function R is critically important in reinforcement learning. It depends on the current state of the world, the action just taken, and the next state of the world:\n",
        "\n",
        "$$\n",
        "r_t = R(s_t, a_t, s_{t+1})\n",
        "$$\n",
        "\n",
        "although frequently this is simplified to just a dependence on the current state, $r_t = R(s_t)$, or state-action pair $r_t = R(s_t,a_t)$.\n",
        "\n",
        "The goal of the agent is to maximize the expected cumulative reward over a trajectory, $\\tau$, but this actually can mean a few things. We’ll notate all of these cases with $R(\\tau)$. The expected return, denoted by $J(\\pi)$, is then:\n",
        "\n",
        "$$\n",
        "J(\\pi) = \\mathbb{E}_{\\tau\\sim \\pi}{R(\\tau)}\n",
        "$$\n",
        "\n",
        "The central optimization problem in RL can then be expressed by\n",
        "\n",
        "$$\n",
        "\\pi^* = \\arg \\max_{\\pi} J(\\pi)\n",
        "$$\n",
        "\n",
        "with $\\pi^*$ being the optimal policy.\n",
        "\n",
        "##### The need to approximate value functions\n",
        "\n",
        "A way to frame the RL problem would be to say that we want actions taken by our agents to maximize the expected achievable value, given a starting state. This can be in the form of a state-value function:\n",
        "\n",
        "$$\n",
        "V^{\\pi}(s) = \\mathbb{E}_{\\tau \\sim \\pi}[R(\\tau) | s_0 = s]\\text{ for states }s \\in S\n",
        "$$\n",
        "\n",
        "or a state-action value function (often called a Q-function):\n",
        "\n",
        "$$\n",
        "Q^{\\pi}(s,a) = \\mathbb{E}_{\\tau \\sim \\pi}[R(\\tau) | s_0 = s, a_0 = a]\\text{ for states }s \\in S\\text{ and actions }a \\in A\n",
        "$$\n",
        "\n",
        "Both value representations obey self-consistency equations called Bellman equations, of which the basic idea is that:\n",
        "\n",
        "    The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next.\n",
        "\n",
        "For a given policy, $\\pi$, the Bellman equations are:\n",
        "\n",
        "$$\n",
        "V^{\\pi}(s_t) = \\mathbb{E}_{a \\sim \\pi}[r(s_t,a_t) + \\gamma V^\\pi(s_{t+1})]\n",
        "$$\n",
        "\n",
        "$$\n",
        "Q^{\\pi}(s_t, a_t) = \\mathbb{E}_{s}[r(s_t,a_t) + \\gamma Q^\\pi(s_{t+1},\\pi(s_{t+1}))]]\n",
        "$$\n",
        "\n",
        "The optimal policy is one that maximizes the expected values $V$ and/or $Q$. This homework will mainly focus on Q-learning techniques, so we are mainly concerned with techniques that consider the latter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CgPG7mFTSg6"
      },
      "source": [
        "#### Some Setup\n",
        "\n",
        "Now for some setup to use during the RL parts of this homework.\n",
        "\n",
        "We will be representing policy and value functions by neural networks. The problems we're working on are fairly simple so we'll be using some simple multi-layer perceptrons (MLPs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNjvGbliTSg7"
      },
      "source": [
        "Run the below if you're starting from a clean environment or suspect you may have the wrong versions of packages \n",
        "\n",
        "*(Also assumes the packages `ipython` and `jupyter` are already installed since you need them anyway to use this notebook)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PuwVe7hTSg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9d6b527-84c0-4701-ad9c-4253f59bc1b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[classic_control]==0.17.3 in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]==0.17.3) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]==0.17.3) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]==0.17.3) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]==0.17.3) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[classic_control]==0.17.3) (0.16.0)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (1.7.1)\n",
            "Collecting nose\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[K     |████████████████████████████████| 154 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy) (1.2.1)\n",
            "Installing collected packages: nose\n",
            "Successfully installed nose-1.3.7\n"
          ]
        }
      ],
      "source": [
        "!pip install \"gym[classic_control]==0.17.3\"\n",
        "!pip install \"torch==1.10.0\"\n",
        "!pip install numpy scipy matplotlib pandas sympy nose seaborn joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2AupbP2TSg8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "import time\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "import typing\n",
        "\n",
        "from logx import EpochLogger\n",
        "from test_policy import load_policy_and_env, run_policy\n",
        "from plot import plot_data, get_datasets\n",
        "\n",
        "def combined_shape(length, shape=None):\n",
        "    if shape is None:\n",
        "        return (length,)\n",
        "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
        "\n",
        "def mlp(sizes, activation, output_activation=nn.Identity):\n",
        "    layers = []\n",
        "    for j in range(len(sizes)-1):\n",
        "        act = activation if j < len(sizes)-2 else output_activation\n",
        "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
        "    return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYTkSNPHTSg9"
      },
      "source": [
        "We also use replay buffers to store trajectories during rollouts so that we can sample from them to aid in training. By storing experiences in replay buffers and sampling from them, we can mitigate catastrophic forgetting (when RL agents forget old experiences).\n",
        "\n",
        "The Replay Buffer mainly keeps a store of the core elements of the Markov Decision Process (MDP) tuple for each interaction of the agent in the training environment. This includes the observed state (obs), the action taken (act), the reward received (rew), the resultant next state after the action (next_obs) and whether the episode was completed/terminated (done).\n",
        "\n",
        "This data can then be sampled during optimization to use for batch updates, as we will do in this homework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDmRGB1cTSg9"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    A simple FIFO experience replay buffer\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 obs_dim: int,\n",
        "                 act_dim: int,\n",
        "                 size: int,\n",
        "                 discrete: bool=False):\n",
        "        self.discrete=discrete\n",
        "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
        "        self.obs2_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
        "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
        "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.ptr, self.size, self.max_size = 0, 0, size\n",
        "\n",
        "    def store(self,\n",
        "              obs: torch.Tensor,\n",
        "              act: int,\n",
        "              rew: float,\n",
        "              next_obs: torch.Tensor,\n",
        "              done: bool):\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.obs2_buf[self.ptr] = next_obs\n",
        "        self.act_buf[self.ptr] = act\n",
        "        self.rew_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr+1) % self.max_size\n",
        "        self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "    def sample_batch(self, batch_size=32):\n",
        "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
        "        batch = dict(obs=self.obs_buf[idxs],\n",
        "                     obs2=self.obs2_buf[idxs],\n",
        "                     act=self.act_buf[idxs],\n",
        "                     rew=self.rew_buf[idxs],\n",
        "                     done=self.done_buf[idxs])\n",
        "        return {k: torch.as_tensor(v, dtype=torch.int64 if (k=='act' and self.discrete) else torch.float32) for k,v in batch.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GZazGKHTSg-"
      },
      "source": [
        "### Q1.1 Deep Q Networks (DQNs)\n",
        "\n",
        "The main idea behind Q-learning is that if we had a function $Q^*:State×Action \\rightarrow \\mathbb{R}$, that could tell us what our return would be, if we were to take an action in a given state, then we could easily construct a policy that maximizes our rewards:\n",
        "\n",
        "$$\n",
        "\\pi^*(s_t) = \\operatorname{argmax}_a Q^*(s_t,a)\n",
        "$$\n",
        "\n",
        "However, we don’t know everything about the world, so we don’t have access to $Q^*$. But, since neural networks are universal function approximators, we can simply create one and train it to resemble $Q^*$.\n",
        "\n",
        "For a discrete action space, a Q-network can be used to estimate the Q-values for each possible action, given a state. The policy can then be designed such that:\n",
        "\n",
        "$$\n",
        "a^\\pi = \\operatorname{argmax}_a Q^\\pi(s_t,a)\n",
        "$$\n",
        "\n",
        "For our training update rule, we’ll use a fact that every $Q$ function for some policy obeys the Bellman equation:\n",
        "\n",
        "$$\n",
        "Q^\\pi(s_t,a) = r + \\gamma Q^\\pi(s_{t+1},\\pi(s_{t+1}))\n",
        "$$\n",
        "\n",
        "The difference between the two sides of the equality is known as the temporal difference error\n",
        "\n",
        "$$\n",
        "\\delta = Q(s_{t},a) - (r + \\gamma \\max_a Q(s_{t+1}, a))\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfpLg2FmTSg_"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, obs_space, act_space, hidden_sizes=(64,64), activation=nn.ReLU, output_activation=nn.Identity):\n",
        "        super().__init__()\n",
        "        self.obs_space = obs_space\n",
        "        self.act_space = act_space\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "\n",
        "        self.Q = mlp([obs_space.shape[0]] + list(hidden_sizes) + [act_space.n],\n",
        "                     activation,\n",
        "                     output_activation)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        # Return output from network scaled to action space limits.\n",
        "        return self.Q(obs)\n",
        "\n",
        "    def act(self, obs):\n",
        "        with torch.no_grad():\n",
        "            return self.Q(obs[None,]).max(1)[1].numpy()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il3jlUueTSg_"
      },
      "source": [
        "We will use the above simple DQN structure to train a policy to solve OpenAI gym's CartPole problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLaSHvKqTShC"
      },
      "outputs": [],
      "source": [
        "# Training hyperparameters\n",
        "steps_per_epoch=4000\n",
        "epochs=20\n",
        "gamma=0.99\n",
        "polyak=0.99  \n",
        "q_lr=1e-3\n",
        "batch_size=100\n",
        "start_steps=10000\n",
        "update_after=1000 \n",
        "update_every=50\n",
        "act_noise=0.1\n",
        "num_test_episodes=10\n",
        "max_ep_len=1000\n",
        "save_freq=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA9jiT1rTShD"
      },
      "source": [
        "We will also define a location for the trained models and logs to get saved. By default, it'll be stored in the current working directory but you may change this "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtYrIcO8TShD",
        "outputId": "c7d40a19-2646-49bf-82b5-06563b877057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Log dir /content/drive/MyDrive/Users/青椒/CS523/hw5_stuffs/dqn already exists! Storing info there anyway.\n",
            "\u001b[32;1mLogging data to /content/drive/MyDrive/Users/青椒/CS523/hw5_stuffs/dqn/progress.txt\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Define the directory location to save dqn logs and models\n",
        "dqn_output_dir = DPATH('dqn')\n",
        "\n",
        "# Logger setup\n",
        "logger_kwargs={'output_dir':dqn_output_dir, 'exp_name':'dqn_CartPole'}\n",
        "logger = EpochLogger(**logger_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL1iQAs5TShE"
      },
      "source": [
        "The problem we'll be working on for DQN is the [CartPole](https://gym.openai.com/envs/CartPole-v1/) OpenAI Gym task. \n",
        "\n",
        "The agent observes the positions and velocities of the cart and pole and can take one of two discrete options to either move the cart to the left, or the right, to try and keep the pole balanced.\n",
        "\n",
        "The episode keeps going for up to 200 steps so long as the pole doesn't fall over too much, and the agent fails if the pole falls more than $\\pm 12$ degrees from the vertical. \n",
        "\n",
        "For every step that it remains 'alive', the agent gets 1 point of reward. The objective is to maximize this reward, which is equivalent to keep the pole balanced for as long as possible.\n",
        "\n",
        "Additional details for the implementation of this environment can be found via the code [source](https://github.com/openai/gym/blob/4ede9280f9c477f1ca09929d10cdc1e1ba1129f1/gym/envs/classic_control/cartpole.py).\n",
        "\n",
        "A random agent would quickly fail to balance the pole, as below (the frame hitches are due to the agent failing and the episode being reset)\n",
        "\n",
        "<img src=\"http://ai.bu.edu/DL523/HW5_files/cartpole_random_demo.gif\" width=\"360em\">\n",
        "\n",
        "But a trained agent can successfully solve this problem (using solution code for this HW).\n",
        "\n",
        "<img src=\"http://ai.bu.edu/DL523/HW5_files/cartpole_demo.gif\" width=\"360em\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NubstBYhTShE"
      },
      "outputs": [],
      "source": [
        "# Environment Definition\n",
        "env_fn = lambda :gym.make('CartPole-v0')\n",
        "env, test_env = env_fn(), env_fn()\n",
        "obs_dim = env.observation_space.shape\n",
        "act_dim = env.action_space.shape\n",
        "\n",
        "# Seeding\n",
        "seed=0\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "env.seed(seed)\n",
        "test_env.seed(seed)\n",
        "\n",
        "# Experience buffer\n",
        "replay_size=int(1e6) \n",
        "replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size, discrete=True)\n",
        "\n",
        "# Create DQN module\n",
        "Qnet = DQN(env.observation_space, env.action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwfVTYMUTShE"
      },
      "source": [
        "To stabilize training we also use a 'target' Q-network which is held relatively constant and is periodically updated with weights from the DQN being trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHdiyWJqTShE"
      },
      "outputs": [],
      "source": [
        "# Create target network\n",
        "Qtarg = deepcopy(Qnet)\n",
        "\n",
        "# Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
        "for p in Qtarg.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Set up model saving\n",
        "logger.setup_pytorch_saver(Qnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMPBiHnlTShF"
      },
      "source": [
        "We can next initialize some helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vy51rZnATShG"
      },
      "outputs": [],
      "source": [
        "# Helper Functions\n",
        "def get_action(o, eps_thresh):\n",
        "    a = Qnet.act(torch.as_tensor(o, dtype=torch.float32))\n",
        "    if np.random.rand() < eps_thresh:\n",
        "        a = env.action_space.sample()\n",
        "    return a\n",
        "\n",
        "def test_agent():\n",
        "    for j in range(num_test_episodes):\n",
        "        o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0\n",
        "        while not(d or (ep_len == max_ep_len)):\n",
        "            # Take deterministic actions at test time (noise_scale=0)\n",
        "            o, r, d, _ = test_env.step(get_action(o, 0))\n",
        "            ep_ret += r\n",
        "            ep_len += 1\n",
        "        logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2LGY50jTShG"
      },
      "source": [
        "### Define DQN Q-learning Loss [10 points]\n",
        "\n",
        "For a sampled batch of data from the replay buffer consisting of sets of observations `o`, corresponding actions taken `a`, achieved reward `r`, resultant observations `o2`, and a flag indicating if the recorded step solved the problem or ended the episode `d`, we compute the DQN loss in 4 main steps:\n",
        "\n",
        "- [x] Compute the Q-network output for `o` and extract Q-values for specific actions taken, $a_t$: $Q(s_t,a_t)$\n",
        "    - Actions, $a_t \\in [0,n-1]$, are indices into a discrete action space with $n$ possible actions.\n",
        "- [x] Next, compute the Bellman backup: $b = r + \\gamma \\max_{a'} Q_{targ}(s_{t+1},a')$\n",
        "    - remember that $\\gamma = 0$ if $d=1$ and that you don't need to track gradients through the target Q-network.\n",
        "- [x] Compute the Bellman error:\n",
        "\n",
        "$$\n",
        "e = Q(s,a) - b\n",
        "$$\n",
        "\n",
        "- [x] Finally compute the Q-loss:\n",
        "\n",
        "$$\n",
        "l_Q = \\frac{1}{N} || e ||_2\n",
        "$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ptD7bOUTShH"
      },
      "outputs": [],
      "source": [
        "import pdb\n",
        "\n",
        "# Set up function for computing DQN Q-loss\n",
        "def compute_loss_q(data: typing.Dict[str, torch.Tensor]):\n",
        "    # Batch data sample:\n",
        "\n",
        "    # O is the observation, a sets of Observation, each line a status\n",
        "    # A is the action\n",
        "    # R is the reward\n",
        "    # o2 is the resultant observations\n",
        "    # d is the done flag, which is False if the episode is not over, and True if it is over\n",
        "    o, a, r, o2, d = data['obs'], data['act'], data['rew'], data['obs2'], data['done']\n",
        "\n",
        "    \"\"\" STUDENT CODE GOES HERE \"\"\"\n",
        "    # Compute q values for observations as appropriate\n",
        "    q = torch.sum(Qnet(o) * F.one_hot(a), dim=1).view(-1, 1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        qtarg_o = Qtarg(o2)\n",
        "        # there's values and indices in torch.max's result.\n",
        "        # here we only need values.\n",
        "        q_target = torch.max(qtarg_o, dim=1).values\n",
        "        backup = r + (1 - d) * gamma * q_target\n",
        "    \n",
        "    loss_q = F.mse_loss(torch.squeeze(q), backup).mean()\n",
        "    \"\"\" STUDENT CODE ENDS \"\"\"\n",
        "\n",
        "    # Useful info for logging\n",
        "    loss_info = dict(QVals=q.detach().numpy())\n",
        "\n",
        "    return loss_q, loss_info\n",
        "\n",
        "# Set up optimizers for policy and q-function\n",
        "q_optimizer = Adam(Qnet.Q.parameters(), lr=q_lr)\n",
        "\n",
        "def update(data):\n",
        "    # First run one gradient descent step for Q.\n",
        "    q_optimizer.zero_grad()\n",
        "    loss_q, loss_info = compute_loss_q(data)\n",
        "    loss_q.backward()\n",
        "    q_optimizer.step()\n",
        "\n",
        "    # Record things\n",
        "    logger.store(LossQ=loss_q.item(), **loss_info)\n",
        "\n",
        "    # Finally, update target networks by polyak averaging.\n",
        "    with torch.no_grad():\n",
        "        for p, p_targ in zip(Qnet.parameters(), Qtarg.parameters()):\n",
        "            # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
        "            # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
        "            p_targ.data.mul_(polyak)\n",
        "            p_targ.data.add_((1 - polyak) * p.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7D3fHRVTShH"
      },
      "source": [
        "Now we run training.\n",
        "\n",
        "While running the code, pay attention the following logged information to determine if things are progressing well:\n",
        "\n",
        "1. For the first few thousand steps, the agents act randomly so you'll likely see low rewards that don't improve until more than `start_steps` steps have elapsed.\n",
        "2. Average Episode and Test Episode returns should generally be increasing (up to 200 which is the max for the CartPole problem)\n",
        "3. Despite optimizing the Q-value loss, the fact that we're having the network learn an arbitrary value means that we may never hit a loss of 0 - the loss should stabilize after some time though (feel free to experiment with more epochs of training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKXYcRa2TShI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a33cc3ad-eac8-4906-97d7-2ca4059b5ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "|             Epoch |               1 |\n",
            "|      AverageEpRet |            22.1 |\n",
            "|          StdEpRet |            11.5 |\n",
            "|          MaxEpRet |              66 |\n",
            "|          MinEpRet |               8 |\n",
            "|  AverageTestEpRet |             176 |\n",
            "|      StdTestEpRet |            20.6 |\n",
            "|      MaxTestEpRet |             200 |\n",
            "|      MinTestEpRet |             141 |\n",
            "|             EpLen |            22.1 |\n",
            "|         TestEpLen |             176 |\n",
            "| TotalEnvInteracts |           4e+03 |\n",
            "|      AverageQVals |            12.5 |\n",
            "|          StdQVals |            8.59 |\n",
            "|          MaxQVals |            33.1 |\n",
            "|          MinQVals |            -3.7 |\n",
            "|             LossQ |           0.487 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |               2 |\n",
            "|      AverageEpRet |            22.8 |\n",
            "|          StdEpRet |            11.3 |\n",
            "|          MaxEpRet |              67 |\n",
            "|          MinEpRet |               9 |\n",
            "|  AverageTestEpRet |             103 |\n",
            "|      StdTestEpRet |            15.5 |\n",
            "|      MaxTestEpRet |             129 |\n",
            "|      MinTestEpRet |              84 |\n",
            "|             EpLen |            22.8 |\n",
            "|         TestEpLen |             103 |\n",
            "| TotalEnvInteracts |           8e+03 |\n",
            "|      AverageQVals |            39.6 |\n",
            "|          StdQVals |              17 |\n",
            "|          MaxQVals |              64 |\n",
            "|          MinQVals |           -15.5 |\n",
            "|             LossQ |            2.26 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |               3 |\n",
            "|      AverageEpRet |            39.5 |\n",
            "|          StdEpRet |            44.7 |\n",
            "|          MaxEpRet |             200 |\n",
            "|          MinEpRet |              10 |\n",
            "|  AverageTestEpRet |             193 |\n",
            "|      StdTestEpRet |            9.27 |\n",
            "|      MaxTestEpRet |             200 |\n",
            "|      MinTestEpRet |             178 |\n",
            "|             EpLen |            39.5 |\n",
            "|         TestEpLen |             193 |\n",
            "| TotalEnvInteracts |         1.2e+04 |\n",
            "|      AverageQVals |            60.5 |\n",
            "|          StdQVals |            23.1 |\n",
            "|          MaxQVals |              97 |\n",
            "|          MinQVals |           -17.9 |\n",
            "|             LossQ |            5.13 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |               4 |\n",
            "|      AverageEpRet |             150 |\n",
            "|          StdEpRet |            47.9 |\n",
            "|          MaxEpRet |             200 |\n",
            "|          MinEpRet |              31 |\n",
            "|  AverageTestEpRet |             193 |\n",
            "|      StdTestEpRet |            8.72 |\n",
            "|      MaxTestEpRet |             200 |\n",
            "|      MinTestEpRet |             174 |\n",
            "|             EpLen |             150 |\n",
            "|         TestEpLen |             193 |\n",
            "| TotalEnvInteracts |         1.6e+04 |\n",
            "|      AverageQVals |            73.2 |\n",
            "|          StdQVals |            25.6 |\n",
            "|          MaxQVals |            96.1 |\n",
            "|          MinQVals |           -19.9 |\n",
            "|             LossQ |            7.14 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |               5 |\n",
            "|      AverageEpRet |             145 |\n",
            "|          StdEpRet |            31.6 |\n",
            "|          MaxEpRet |             200 |\n",
            "|          MinEpRet |              89 |\n",
            "|  AverageTestEpRet |             152 |\n",
            "|      StdTestEpRet |            11.6 |\n",
            "|      MaxTestEpRet |             176 |\n",
            "|      MinTestEpRet |             136 |\n",
            "|             EpLen |             145 |\n",
            "|         TestEpLen |             152 |\n",
            "| TotalEnvInteracts |           2e+04 |\n",
            "|      AverageQVals |            79.9 |\n",
            "|          StdQVals |            28.7 |\n",
            "|          MaxQVals |             104 |\n",
            "|          MinQVals |           -18.3 |\n",
            "|             LossQ |            7.88 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |               6 |\n",
            "|      AverageEpRet |             174 |\n",
            "|          StdEpRet |            27.1 |\n",
            "|          MaxEpRet |             200 |\n",
            "|          MinEpRet |             122 |\n",
            "|  AverageTestEpRet |             200 |\n",
            "|      StdTestEpRet |               0 |\n",
            "|      MaxTestEpRet |             200 |\n",
            "|      MinTestEpRet |             200 |\n",
            "|             EpLen |             174 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         2.4e+04 |\n",
            "|      AverageQVals |            82.6 |\n",
            "|          StdQVals |            32.1 |\n",
            "|          MaxQVals |             114 |\n",
            "|          MinQVals |           -18.2 |\n",
            "|             LossQ |            7.33 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |               7 |\n",
            "|      AverageEpRet |             195 |\n",
            "|          StdEpRet |            15.6 |\n",
            "|          MaxEpRet |             200 |\n",
            "|          MinEpRet |             138 |\n",
            "|  AverageTestEpRet |             200 |\n",
            "|      StdTestEpRet |               0 |\n",
            "|      MaxTestEpRet |             200 |\n",
            "|      MinTestEpRet |             200 |\n",
            "|             EpLen |             195 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         2.8e+04 |\n",
            "|      AverageQVals |            86.8 |\n",
            "|          StdQVals |            33.7 |\n",
            "|          MaxQVals |             122 |\n",
            "|          MinQVals |           -14.6 |\n",
            "|             LossQ |            12.5 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |               8 |\n",
            "|      AverageEpRet |             195 |\n",
            "|          StdEpRet |            23.6 |\n",
            "|          MaxEpRet |             200 |\n",
            "|          MinEpRet |              89 |\n",
            "|  AverageTestEpRet |             200 |\n",
            "|      StdTestEpRet |               0 |\n",
            "|      MaxTestEpRet |             200 |\n",
            "|      MinTestEpRet |             200 |\n",
            "|             EpLen |             195 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         3.2e+04 |\n",
            "|      AverageQVals |            88.5 |\n",
            "|          StdQVals |            32.9 |\n",
            "|          MaxQVals |             126 |\n",
            "|          MinQVals |           -13.7 |\n",
            "|             LossQ |            14.2 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |               9 |\n",
            "|      AverageEpRet |             191 |\n",
            "|          StdEpRet |            38.5 |\n",
            "|          MaxEpRet |             200 |\n",
            "|          MinEpRet |              19 |\n",
            "|  AverageTestEpRet |             200 |\n",
            "|      StdTestEpRet |               0 |\n",
            "|      MaxTestEpRet |             200 |\n",
            "|      MinTestEpRet |             200 |\n",
            "|             EpLen |             191 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         3.6e+04 |\n",
            "|      AverageQVals |              91 |\n",
            "|          StdQVals |            30.9 |\n",
            "|          MaxQVals |             125 |\n",
            "|          MinQVals |           -13.1 |\n",
            "|             LossQ |            19.4 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              10 |\n",
            "|      AverageEpRet |             188 |\n",
            "|          StdEpRet |            35.1 |\n",
            "|          MaxEpRet |             200 |\n",
            "|          MinEpRet |              39 |\n",
            "|  AverageTestEpRet |             200 |\n",
            "|      StdTestEpRet |               0 |\n",
            "|      MaxTestEpRet |             200 |\n",
            "|      MinTestEpRet |             200 |\n",
            "|             EpLen |             188 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |           4e+04 |\n",
            "|      AverageQVals |            90.5 |\n",
            "|          StdQVals |            28.5 |\n",
            "|          MaxQVals |             122 |\n",
            "|          MinQVals |           -12.5 |\n",
            "|             LossQ |            21.9 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              11 |\n",
            "|      AverageEpRet |             192 |\n",
            "|          StdEpRet |            15.5 |\n",
            "|          MaxEpRet |             200 |\n",
            "|          MinEpRet |             153 |\n",
            "|  AverageTestEpRet |             200 |\n",
            "|      StdTestEpRet |               0 |\n",
            "|      MaxTestEpRet |             200 |\n",
            "|      MinTestEpRet |             200 |\n",
            "|             EpLen |             192 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         4.4e+04 |\n",
            "|      AverageQVals |            89.3 |\n",
            "|          StdQVals |            26.5 |\n",
            "|          MaxQVals |             118 |\n",
            "|          MinQVals |           -13.2 |\n",
            "|             LossQ |            20.8 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              12 |\n",
            "|      AverageEpRet |             179 |\n",
            "|          StdEpRet |              29 |\n",
            "|          MaxEpRet |             200 |\n",
            "|          MinEpRet |             110 |\n",
            "|  AverageTestEpRet |             181 |\n",
            "|      StdTestEpRet |              28 |\n",
            "|      MaxTestEpRet |             200 |\n",
            "|      MinTestEpRet |             109 |\n",
            "|             EpLen |             179 |\n",
            "|         TestEpLen |             181 |\n",
            "| TotalEnvInteracts |         4.8e+04 |\n",
            "|      AverageQVals |            88.1 |\n",
            "|          StdQVals |            25.3 |\n",
            "|          MaxQVals |             115 |\n",
            "|          MinQVals |           -9.53 |\n",
            "|             LossQ |            19.4 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              13 |\n",
            "|      AverageEpRet |             178 |\n",
            "|          StdEpRet |            33.7 |\n",
            "|          MaxEpRet |             200 |\n",
            "|          MinEpRet |             101 |\n",
            "|  AverageTestEpRet |             200 |\n",
            "|      StdTestEpRet |               0 |\n",
            "|      MaxTestEpRet |             200 |\n",
            "|      MinTestEpRet |             200 |\n",
            "|             EpLen |             178 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         5.2e+04 |\n",
            "|      AverageQVals |            86.5 |\n",
            "|          StdQVals |            24.9 |\n",
            "|          MaxQVals |             112 |\n",
            "|          MinQVals |           -9.09 |\n",
            "|             LossQ |            20.9 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              14 |\n",
            "|      AverageEpRet |             183 |\n",
            "|          StdEpRet |            36.6 |\n",
            "|          MaxEpRet |             200 |\n",
            "|          MinEpRet |              41 |\n",
            "|  AverageTestEpRet |             200 |\n",
            "|      StdTestEpRet |               0 |\n",
            "|      MaxTestEpRet |             200 |\n",
            "|      MinTestEpRet |             200 |\n",
            "|             EpLen |             183 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         5.6e+04 |\n",
            "|      AverageQVals |            85.9 |\n",
            "|          StdQVals |            24.3 |\n",
            "|          MaxQVals |             111 |\n",
            "|          MinQVals |           -10.1 |\n",
            "|             LossQ |            20.2 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              15 |\n",
            "|      AverageEpRet |             194 |\n",
            "|          StdEpRet |            18.4 |\n",
            "|          MaxEpRet |             200 |\n",
            "|          MinEpRet |             116 |\n",
            "|  AverageTestEpRet |             200 |\n",
            "|      StdTestEpRet |               0 |\n",
            "|      MaxTestEpRet |             200 |\n",
            "|      MinTestEpRet |             200 |\n",
            "|             EpLen |             194 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |           6e+04 |\n",
            "|      AverageQVals |            85.5 |\n",
            "|          StdQVals |            23.5 |\n",
            "|          MaxQVals |             108 |\n",
            "|          MinQVals |           -9.62 |\n",
            "|             LossQ |            20.3 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              16 |\n",
            "|      AverageEpRet |             190 |\n",
            "|          StdEpRet |            26.3 |\n",
            "|          MaxEpRet |             200 |\n",
            "|          MinEpRet |             110 |\n",
            "|  AverageTestEpRet |             200 |\n",
            "|      StdTestEpRet |               0 |\n",
            "|      MaxTestEpRet |             200 |\n",
            "|      MinTestEpRet |             200 |\n",
            "|             EpLen |             190 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         6.4e+04 |\n",
            "|      AverageQVals |            84.9 |\n",
            "|          StdQVals |            22.9 |\n",
            "|          MaxQVals |             107 |\n",
            "|          MinQVals |              -9 |\n",
            "|             LossQ |            21.2 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              17 |\n",
            "|      AverageEpRet |             178 |\n",
            "|          StdEpRet |            35.7 |\n",
            "|          MaxEpRet |             200 |\n",
            "|          MinEpRet |              95 |\n",
            "|  AverageTestEpRet |             200 |\n",
            "|      StdTestEpRet |               0 |\n",
            "|      MaxTestEpRet |             200 |\n",
            "|      MinTestEpRet |             200 |\n",
            "|             EpLen |             178 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         6.8e+04 |\n",
            "|      AverageQVals |            84.6 |\n",
            "|          StdQVals |            22.5 |\n",
            "|          MaxQVals |             107 |\n",
            "|          MinQVals |           -9.31 |\n",
            "|             LossQ |            20.8 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              18 |\n",
            "|      AverageEpRet |             178 |\n",
            "|          StdEpRet |            38.4 |\n",
            "|          MaxEpRet |             200 |\n",
            "|          MinEpRet |              55 |\n",
            "|  AverageTestEpRet |             200 |\n",
            "|      StdTestEpRet |               0 |\n",
            "|      MaxTestEpRet |             200 |\n",
            "|      MinTestEpRet |             200 |\n",
            "|             EpLen |             178 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         7.2e+04 |\n",
            "|      AverageQVals |            84.9 |\n",
            "|          StdQVals |            22.3 |\n",
            "|          MaxQVals |             108 |\n",
            "|          MinQVals |           -11.1 |\n",
            "|             LossQ |            21.3 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              19 |\n",
            "|      AverageEpRet |             197 |\n",
            "|          StdEpRet |            13.3 |\n",
            "|          MaxEpRet |             200 |\n",
            "|          MinEpRet |             139 |\n",
            "|  AverageTestEpRet |             200 |\n",
            "|      StdTestEpRet |             1.5 |\n",
            "|      MaxTestEpRet |             200 |\n",
            "|      MinTestEpRet |             195 |\n",
            "|             EpLen |             197 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         7.6e+04 |\n",
            "|      AverageQVals |            86.3 |\n",
            "|          StdQVals |            22.3 |\n",
            "|          MaxQVals |             108 |\n",
            "|          MinQVals |           -11.3 |\n",
            "|             LossQ |            21.9 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              20 |\n",
            "|      AverageEpRet |             184 |\n",
            "|          StdEpRet |            42.4 |\n",
            "|          MaxEpRet |             200 |\n",
            "|          MinEpRet |              12 |\n",
            "|  AverageTestEpRet |             200 |\n",
            "|      StdTestEpRet |               0 |\n",
            "|      MaxTestEpRet |             200 |\n",
            "|      MinTestEpRet |             200 |\n",
            "|             EpLen |             184 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |           8e+04 |\n",
            "|      AverageQVals |            85.5 |\n",
            "|          StdQVals |            22.5 |\n",
            "|          MaxQVals |             107 |\n",
            "|          MinQVals |           -10.7 |\n",
            "|             LossQ |            22.6 |\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Prepare for interaction with environment\n",
        "total_steps = steps_per_epoch * epochs\n",
        "o, ep_ret, ep_len = env.reset(), 0, 0\n",
        "\n",
        "# Main loop: collect experience in env and update/log each epoch\n",
        "for t in range(total_steps):\n",
        "    \n",
        "    # Until start_steps have elapsed, randomly sample actions\n",
        "    # from a uniform distribution for better exploration. Afterwards, \n",
        "    # use the learned policy (with some noise, via act_noise). \n",
        "    if t > start_steps: \n",
        "        a = get_action(o, act_noise)\n",
        "    else:\n",
        "        a = env.action_space.sample()\n",
        "\n",
        "    # Step the env\n",
        "    o2, r, d, _ = env.step(a)\n",
        "    ep_ret += r\n",
        "    ep_len += 1\n",
        "\n",
        "    # Ignore the \"done\" signal if it comes from hitting the time\n",
        "    # horizon (that is, when it's an artificial terminal signal\n",
        "    # that isn't based on the agent's state)\n",
        "    d = False if ep_len==max_ep_len else d\n",
        "\n",
        "    # Store experience to replay buffer\n",
        "    replay_buffer.store(o, a, r, o2, d)\n",
        "\n",
        "    # Super critical, easy to overlook step: make sure to update \n",
        "    # most recent observation!\n",
        "    o = o2\n",
        "\n",
        "    # End of trajectory handling\n",
        "    if d or (ep_len == max_ep_len):\n",
        "        logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
        "        o, ep_ret, ep_len = env.reset(), 0, 0\n",
        "\n",
        "    # Update handling\n",
        "    if t >= update_after and t % update_every == 0:\n",
        "        for _ in range(update_every):\n",
        "            batch = replay_buffer.sample_batch(batch_size)\n",
        "            update(data=batch)\n",
        "\n",
        "    # End of epoch handling\n",
        "    if (t+1) % steps_per_epoch == 0:\n",
        "        epoch = (t+1) // steps_per_epoch\n",
        "\n",
        "        # Save model\n",
        "        if (epoch % save_freq == 0) or (epoch == epochs):\n",
        "            logger.save_state({'env': env}, None)\n",
        "\n",
        "        # Test the performance of the deterministic version of the agent.\n",
        "        test_agent()\n",
        "\n",
        "        # Log info about epoch\n",
        "        logger.log_tabular('Epoch', epoch)\n",
        "        logger.log_tabular('EpRet', with_min_and_max=True)\n",
        "        logger.log_tabular('TestEpRet', with_min_and_max=True)\n",
        "        logger.log_tabular('EpLen', average_only=True)\n",
        "        logger.log_tabular('TestEpLen', average_only=True)\n",
        "        logger.log_tabular('TotalEnvInteracts', t)\n",
        "        logger.log_tabular('QVals', with_min_and_max=True)\n",
        "        logger.log_tabular('LossQ', average_only=True)\n",
        "        logger.dump_tabular()\n",
        "\n",
        "env.close()\n",
        "test_env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDyuNGX9TShI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "e76bd3ac-0703-4201-bde5-93b9f517b610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No file named config.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure()\n",
        "data = get_datasets(dqn_output_dir)\n",
        "plot_data(data, xaxis='TotalEnvInteracts', value='Performance', smooth=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-PCdPttTShI"
      },
      "source": [
        "We can also run the trained agents as below (set render=True to visualize - note that additional settings may be required to view on Colab or on a remote server)\n",
        "\n",
        "Note: In order for this code to work, the DQN model definition should be in scope.\n",
        "\n",
        "The solved CartPole controller would typically have EpRet close to 200 (where 200 is the max) - If it doesn't quite plateau there, try rerunning the training (we're only training it for a short while with only sligtly tuned hyperparameters so it's possible that it may not stably plateau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ev2ZxtvNTShI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0198b8e-cb1f-492a-f1ca-579abbea3696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Loading from /content/drive/MyDrive/Users/青椒/CS523/hw5_stuffs/dqn/pyt_save/model.pt.\n",
            "\n",
            "\n",
            "\u001b[32;1mLogging data to /tmp/experiments/1650459421/progress.txt\u001b[0m\n",
            "Episode 0 \t EpRet 200.000 \t EpLen 200\n",
            "Episode 1 \t EpRet 200.000 \t EpLen 200\n",
            "Episode 2 \t EpRet 200.000 \t EpLen 200\n",
            "Episode 3 \t EpRet 200.000 \t EpLen 200\n",
            "Episode 4 \t EpRet 200.000 \t EpLen 200\n",
            "Episode 5 \t EpRet 200.000 \t EpLen 200\n",
            "Episode 6 \t EpRet 200.000 \t EpLen 200\n",
            "Episode 7 \t EpRet 200.000 \t EpLen 200\n",
            "Episode 8 \t EpRet 200.000 \t EpLen 200\n",
            "Episode 9 \t EpRet 200.000 \t EpLen 200\n",
            "-------------------------------------\n",
            "|    AverageEpRet |             200 |\n",
            "|        StdEpRet |               0 |\n",
            "|        MaxEpRet |             200 |\n",
            "|        MinEpRet |             200 |\n",
            "|           EpLen |             200 |\n",
            "-------------------------------------\n"
          ]
        }
      ],
      "source": [
        "playback_env, get_action = load_policy_and_env(dqn_output_dir, 'last', True)\n",
        "run_policy(playback_env, get_action, num_episodes=10, render=False)\n",
        "playback_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnJyBgrKTShI"
      },
      "source": [
        "### Q1.2 Deep Deterministic Policy Gradient (DDPG) - An Actor-Critic Algorithm\n",
        "\n",
        "Next, we'll implement a Q-learning algorithm for continuous control using a popular actor-critic algortihm, DDPG.\n",
        "\n",
        "One of the main traits of actor-critic algortihsm is that unlike with DQNs:\n",
        "1. The policy network and value estimation network are separated into two separate, distinct functions\n",
        "2. They can be used to solve continuous controls problems, unlike DQNs which are limited to discrete action spaces.\n",
        "\n",
        "The Q-learning side of training the critic works similarly to DQNs, except that Q-networks now take states and actions both as inputs to estimate a Q-value, as opposed to simultaneously estimating Q values over all possible discrete actions.\n",
        "\n",
        "The Policy learning in DDPG is fairly simple. We want to learn a deterministic policy $\\pi_{\\theta}(s)$ (parameterized by $\\theta$) which gives the action that maximizes $Q_{\\phi}(s,a)$, a Q-value critic parameterized by $\\phi$. Because the action space is continuous, and we assume the Q-function is differentiable with respect to action, we can just perform gradient ascent (with respect to policy parameters only) to solve\n",
        "\n",
        "$\\max_{\\theta} \\mathbb{E}_{s \\sim {\\mathcal D}} [ Q_{\\phi}(s, \\pi_{\\theta}(s)) ]$\n",
        "\n",
        "Note that the Q-function parameters are treated as constants here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz5gREZwTShJ"
      },
      "outputs": [],
      "source": [
        "class MLPActor(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):\n",
        "        super().__init__()\n",
        "        pi_sizes = [obs_dim] + list(hidden_sizes) + [act_dim]\n",
        "        self.pi = mlp(pi_sizes, activation, nn.Tanh)\n",
        "        self.act_limit = act_limit\n",
        "\n",
        "    def forward(self, obs):\n",
        "        # Return output from network scaled to action space limits.\n",
        "        return self.act_limit * self.pi(obs)\n",
        "\n",
        "class MLPQFunction(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
        "        super().__init__()\n",
        "        self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
        "\n",
        "    def forward(self, obs, act):\n",
        "        q = self.q(torch.cat([obs, act], dim=-1))\n",
        "        return torch.squeeze(q, -1) # Critical to ensure q has right shape.\n",
        "\n",
        "class MLPActorCritic(nn.Module):\n",
        "\n",
        "    def __init__(self, observation_space, action_space, hidden_sizes=(256,256),\n",
        "                 activation=nn.ReLU):\n",
        "        super().__init__()\n",
        "\n",
        "        obs_dim = observation_space.shape[0]\n",
        "        act_dim = action_space.shape[0]\n",
        "        act_limit = action_space.high[0]\n",
        "\n",
        "        # build policy and value functions\n",
        "        self.pi = MLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit)\n",
        "        self.q = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation)\n",
        "\n",
        "    def act(self, obs):\n",
        "        with torch.no_grad():\n",
        "            return self.pi(obs).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNxuRZlBTShJ"
      },
      "source": [
        "For this part of the homework, we'll test DDPG on a simple inverted pendulum problem.\n",
        "\n",
        "As before, in addition to the main trained networks, we build additional target networks to stabilize training.\n",
        "\n",
        "We will also set the logging directory to be a part of the current working directory but feel free to change that as necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAhxwT7QTShJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53360f94-cd11-405d-e9ec-8c91107dc4e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Log dir /content/drive/MyDrive/Users/青椒/CS523/hw5_stuffs/ddpg already exists! Storing info there anyway.\n",
            "\u001b[32;1mLogging data to /content/drive/MyDrive/Users/青椒/CS523/hw5_stuffs/ddpg/progress.txt\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Define the directory location to save DDPG logs and models\n",
        "ddpg_output_dir = DPATH('ddpg')\n",
        "\n",
        "# Logger setup\n",
        "logger_kwargs={'output_dir': ddpg_output_dir, 'exp_name':'ddpg_pendulum'}\n",
        "logger = EpochLogger(**logger_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0J-IdHpTShJ"
      },
      "source": [
        "The problem we'll be working on for DDPG is the (inverted) [Pendulum](https://gym.openai.com/envs/CartPole-v1/) OpenAI Gym task. \n",
        "\n",
        "The agent observes the angular position and velocity of the pendulum and can apply a continuous-valued torque to try and balance the pendulum.\n",
        "\n",
        "The episode keeps going for up to 200 steps and every step the agent gets reward mainly proportional to how far off it is from balancing the inverted pendulum, with smaller reward components related to how fast the pendulum is moving and how much torque is being applied. The objective is to maximize this reward, which is equivalent to balancing the inverted pendulum quickly and for as long as possible.\n",
        "\n",
        "Additional details for the implementation of this environment can be found via the code [source](https://github.com/openai/gym/blob/4ede9280f9c477f1ca09929d10cdc1e1ba1129f1/gym/envs/classic_control/pendulum.py).\n",
        "\n",
        "A random agent (or a poorly trained one) is unlikely to successfully balance the pendulum, as seen below:\n",
        "\n",
        "<img src=\"http://ai.bu.edu/DL523/HW5_files/pendulum_random_demo.gif\" width=\"360em\">\n",
        "\n",
        "But a well trained agent can successfully solve this problem (using solution code for this HW).\n",
        "\n",
        "<img src=\"http://ai.bu.edu/DL523/HW5_files/pendulum_demo.gif\" width=\"360em\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC8dZvZlTShM"
      },
      "outputs": [],
      "source": [
        "# Training hyperparameters\n",
        "steps_per_epoch=4000\n",
        "epochs=20\n",
        "gamma=0.99\n",
        "polyak=0.995\n",
        "batch_size=100\n",
        "start_steps=10000\n",
        "update_after=1000 \n",
        "update_every=50\n",
        "act_noise=0.1\n",
        "num_test_episodes=10\n",
        "max_ep_len=1000\n",
        "save_freq=1\n",
        "\n",
        "pi_lr=1e-3 \n",
        "q_lr=1e-3\n",
        "\n",
        "# Environment Definition\n",
        "env_fn = lambda :gym.make('Pendulum-v0')\n",
        "env, test_env = env_fn(), env_fn()\n",
        "obs_dim = env.observation_space.shape\n",
        "act_dim = env.action_space.shape[0]\n",
        "# Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
        "act_limit = env.action_space.high[0]\n",
        "\n",
        "# Seeding\n",
        "seed=0\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "env.seed(seed)\n",
        "test_env.seed(seed)\n",
        "\n",
        "# Experience buffer\n",
        "replay_size=int(1e6) \n",
        "replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n",
        "\n",
        "# Create actor-critic module and target networks\n",
        "ac = MLPActorCritic(env.observation_space, env.action_space)\n",
        "ac_targ = deepcopy(ac)\n",
        "\n",
        "# Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
        "for p in ac_targ.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Set up model saving\n",
        "logger.setup_pytorch_saver(ac)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHLGGc5WTShN"
      },
      "source": [
        "We can next initialize modified helper functions - similar to the ones before but setup for the different architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjxtbzXNTShO"
      },
      "outputs": [],
      "source": [
        "# Helper Functions\n",
        "def get_action(o, noise_scale):\n",
        "    a = ac.act(torch.as_tensor(o, dtype=torch.float32))\n",
        "    a += noise_scale * np.random.randn(act_dim)\n",
        "    return np.clip(a, -act_limit, act_limit)\n",
        "\n",
        "def test_agent():\n",
        "    for j in range(num_test_episodes):\n",
        "        o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0\n",
        "        while not(d or (ep_len == max_ep_len)):\n",
        "            # Take deterministic actions at test time (noise_scale=0)\n",
        "            o, r, d, _ = test_env.step(get_action(o, 0))\n",
        "            ep_ret += r\n",
        "            ep_len += 1\n",
        "        logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAiyEYlBTShO"
      },
      "source": [
        "### Define DDPG Q-learning Loss [10 points]\n",
        "\n",
        "This part is similar to DQN, except you'll need to modify your code to make use of the actor-critic separation appropriately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxlhbJNqTShO"
      },
      "outputs": [],
      "source": [
        "# Set up function for computing DDPG Q-loss\n",
        "def compute_loss_q(data):\n",
        "    o, a, r, o2, d = data['obs'], data['act'], data['rew'], data['obs2'], data['done']\n",
        "\n",
        "    \"\"\" STUDENT CODE GOES HERE \"\"\"\n",
        "    q = ac.q(o, a)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # first we compute the given act for using act_target's Policy\n",
        "        a2 = ac_targ.pi(o2)\n",
        "        ac_targ_q = ac_targ.q(o2, a2)\n",
        "        backup = r + (1 - d) * gamma * ac_targ_q\n",
        "\n",
        "    loss_q = F.mse_loss(q, backup).mean()\n",
        "    \"\"\" STUDENT CODE ENDS \"\"\"\n",
        "\n",
        "    # Useful info for logging\n",
        "    loss_info = dict(QVals=q.detach().numpy())\n",
        "\n",
        "    return loss_q, loss_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVFk4hN1TShQ"
      },
      "source": [
        "### Define DDPG Policy $\\pi$ Loss [10 points]\n",
        "\n",
        "The policy optimization loss is simple to compute. It can be computed as: $L_\\pi = -Q(o,\\pi(o))$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NltXTDjWTShQ"
      },
      "outputs": [],
      "source": [
        "# Set up function for computing DDPG pi loss\n",
        "def compute_loss_pi(data):\n",
        "    o = data['obs']\n",
        "    \"\"\" STUDENT CODE GOES HERE \"\"\"\n",
        "    pi_loss = -ac.q(o, ac.pi(o)).mean()\n",
        "    \"\"\" STUDENT CODE ENDS \"\"\"\n",
        "    return pi_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYN3kyI1TShR"
      },
      "source": [
        "The overall updates are handled similarly to DQN before"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class model_no_grad():\n",
        "    def __init__(self, model: nn.Module):\n",
        "        self.model = model\n",
        "\n",
        "    def __enter__(self):\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def __exit__(self, exc_type: any, exc_value: any, traceback: any) -> None:\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = True"
      ],
      "metadata": {
        "id": "hqzi7xMCgCQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbShf5xXTShR"
      },
      "outputs": [],
      "source": [
        "# Set up optimizers for policy and q-function\n",
        "pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
        "q_optimizer = Adam(ac.q.parameters(), lr=q_lr)\n",
        "\n",
        "# Set up model saving\n",
        "logger.setup_pytorch_saver(ac)\n",
        "\n",
        "def update(data):\n",
        "    # First run one gradient descent step for Q.\n",
        "    q_optimizer.zero_grad()\n",
        "    loss_q, loss_info = compute_loss_q(data)\n",
        "    loss_q.backward()\n",
        "    q_optimizer.step()\n",
        "\n",
        "    # Freeze Q-network so you don't waste computational effort \n",
        "    # computing gradients for it during the policy learning step.\n",
        "    # after exit,  Unfreeze Q-network so you can optimize it at next DDPG step.\n",
        "    with model_no_grad(ac.q):\n",
        "        # Next run one gradient descent step for pi.\n",
        "        pi_optimizer.zero_grad()\n",
        "        loss_pi = compute_loss_pi(data)\n",
        "        loss_pi.backward()\n",
        "        pi_optimizer.step()\n",
        "\n",
        "    # Record things\n",
        "    logger.store(LossQ=loss_q.item(), LossPi=loss_pi.item(), **loss_info)\n",
        "\n",
        "    # Finally, update target networks by polyak averaging.\n",
        "    with torch.no_grad():\n",
        "        for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
        "            # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
        "            # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
        "            p_targ.data.mul_(polyak)\n",
        "            p_targ.data.add_((1 - polyak) * p.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5JlkMc7TShR"
      },
      "source": [
        "Now we run training.\n",
        "\n",
        "While running the code, pay attention the following logged information to determine if things are progressing well:\n",
        "\n",
        "1. For the first few thousand steps, the agents act randomly so you'll likely see low rewards that don't improve until more than `start_steps` steps have elapsed.\n",
        "2. Average Episode and Test Episode returns should generally be increasing from a very negative value to an average of > -150 for a successful agent.\n",
        "3. The policy loss (LossPi in the log as computed in `compute_loss_pi`) should decrease as the agent gets beter as this reflects how good the policy is at choosing 'good' actions.\n",
        "4. As before, we also expect the Q-value loss to decrease as the agent gets better at estimating Q-values.\n",
        "\n",
        "Unlike DQNs, where everything rides on a good estimation of Q-values, the policy behavior for actor-critic learning relies on critics learning useful Q-value estimations and actors learning useful policies based on the estimated Q-values. \n",
        "The separation offers increased flexibility and often improved representational power (in addition to being able to support continuous-valued actions) but does add extra knobs to tune."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f3iUB6VTShS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "745d9291-6ae0-4536-caa9-db4feb990a69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "|             Epoch |               1 |\n",
            "|      AverageEpRet |       -1.23e+03 |\n",
            "|          StdEpRet |             338 |\n",
            "|          MaxEpRet |            -759 |\n",
            "|          MinEpRet |       -1.75e+03 |\n",
            "|  AverageTestEpRet |            -293 |\n",
            "|      StdTestEpRet |             220 |\n",
            "|      MaxTestEpRet |          -0.441 |\n",
            "|      MinTestEpRet |            -728 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |           4e+03 |\n",
            "|      AverageQVals |           -39.5 |\n",
            "|          StdQVals |            28.7 |\n",
            "|          MaxQVals |            1.86 |\n",
            "|          MinQVals |            -113 |\n",
            "|            LossPi |            38.4 |\n",
            "|             LossQ |             9.3 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |               2 |\n",
            "|      AverageEpRet |       -1.29e+03 |\n",
            "|          StdEpRet |             308 |\n",
            "|          MaxEpRet |            -849 |\n",
            "|          MinEpRet |       -1.76e+03 |\n",
            "|  AverageTestEpRet |           -97.4 |\n",
            "|      StdTestEpRet |            90.2 |\n",
            "|      MaxTestEpRet |          -0.577 |\n",
            "|      MinTestEpRet |            -250 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |           8e+03 |\n",
            "|      AverageQVals |           -91.1 |\n",
            "|          StdQVals |            43.4 |\n",
            "|          MaxQVals |            46.5 |\n",
            "|          MinQVals |            -190 |\n",
            "|            LossPi |            87.7 |\n",
            "|             LossQ |              44 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |               3 |\n",
            "|      AverageEpRet |            -662 |\n",
            "|          StdEpRet |             570 |\n",
            "|          MaxEpRet |            -115 |\n",
            "|          MinEpRet |       -1.61e+03 |\n",
            "|  AverageTestEpRet |            -223 |\n",
            "|      StdTestEpRet |            90.8 |\n",
            "|      MaxTestEpRet |            -123 |\n",
            "|      MinTestEpRet |            -350 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         1.2e+04 |\n",
            "|      AverageQVals |           -90.6 |\n",
            "|          StdQVals |            76.5 |\n",
            "|          MaxQVals |            94.7 |\n",
            "|          MinQVals |            -231 |\n",
            "|            LossPi |            85.3 |\n",
            "|             LossQ |            60.6 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |               4 |\n",
            "|      AverageEpRet |            -131 |\n",
            "|          StdEpRet |            86.7 |\n",
            "|          MaxEpRet |           -2.65 |\n",
            "|          MinEpRet |            -336 |\n",
            "|  AverageTestEpRet |            -155 |\n",
            "|      StdTestEpRet |            91.5 |\n",
            "|      MaxTestEpRet |              -3 |\n",
            "|      MinTestEpRet |            -246 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         1.6e+04 |\n",
            "|      AverageQVals |           -41.2 |\n",
            "|          StdQVals |            94.7 |\n",
            "|          MaxQVals |            94.8 |\n",
            "|          MinQVals |            -235 |\n",
            "|            LossPi |            36.4 |\n",
            "|             LossQ |            47.5 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |               5 |\n",
            "|      AverageEpRet |            -158 |\n",
            "|          StdEpRet |            78.1 |\n",
            "|          MaxEpRet |           -1.84 |\n",
            "|          MinEpRet |            -342 |\n",
            "|  AverageTestEpRet |            -120 |\n",
            "|      StdTestEpRet |             105 |\n",
            "|      MaxTestEpRet |           -1.48 |\n",
            "|      MinTestEpRet |            -347 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |           2e+04 |\n",
            "|      AverageQVals |           -19.2 |\n",
            "|          StdQVals |            79.7 |\n",
            "|          MaxQVals |            76.8 |\n",
            "|          MinQVals |            -229 |\n",
            "|            LossPi |            16.1 |\n",
            "|             LossQ |            27.6 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |               6 |\n",
            "|      AverageEpRet |            -169 |\n",
            "|          StdEpRet |            58.6 |\n",
            "|          MaxEpRet |            -117 |\n",
            "|          MinEpRet |            -260 |\n",
            "|  AverageTestEpRet |            -143 |\n",
            "|      StdTestEpRet |            45.8 |\n",
            "|      MaxTestEpRet |            -116 |\n",
            "|      MinTestEpRet |            -236 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         2.4e+04 |\n",
            "|      AverageQVals |           -18.3 |\n",
            "|          StdQVals |            70.7 |\n",
            "|          MaxQVals |            52.4 |\n",
            "|          MinQVals |            -208 |\n",
            "|            LossPi |            15.9 |\n",
            "|             LossQ |            21.3 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |               7 |\n",
            "|      AverageEpRet |            -163 |\n",
            "|          StdEpRet |            66.5 |\n",
            "|          MaxEpRet |           -2.78 |\n",
            "|          MinEpRet |            -258 |\n",
            "|  AverageTestEpRet |            -131 |\n",
            "|      StdTestEpRet |            61.5 |\n",
            "|      MaxTestEpRet |           -1.46 |\n",
            "|      MinTestEpRet |            -234 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         2.8e+04 |\n",
            "|      AverageQVals |           -19.8 |\n",
            "|          StdQVals |            68.9 |\n",
            "|          MaxQVals |            44.6 |\n",
            "|          MinQVals |            -207 |\n",
            "|            LossPi |            17.8 |\n",
            "|             LossQ |            19.7 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |               8 |\n",
            "|      AverageEpRet |            -143 |\n",
            "|          StdEpRet |            85.5 |\n",
            "|          MaxEpRet |           -1.13 |\n",
            "|          MinEpRet |            -338 |\n",
            "|  AverageTestEpRet |            -108 |\n",
            "|      StdTestEpRet |            61.7 |\n",
            "|      MaxTestEpRet |           -1.85 |\n",
            "|      MinTestEpRet |            -227 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         3.2e+04 |\n",
            "|      AverageQVals |           -20.7 |\n",
            "|          StdQVals |            67.7 |\n",
            "|          MaxQVals |              36 |\n",
            "|          MinQVals |            -216 |\n",
            "|            LossPi |              19 |\n",
            "|             LossQ |            20.2 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |               9 |\n",
            "|      AverageEpRet |            -178 |\n",
            "|          StdEpRet |            89.3 |\n",
            "|          MaxEpRet |           -3.59 |\n",
            "|          MinEpRet |            -356 |\n",
            "|  AverageTestEpRet |            -170 |\n",
            "|      StdTestEpRet |             104 |\n",
            "|      MaxTestEpRet |            -2.8 |\n",
            "|      MinTestEpRet |            -348 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         3.6e+04 |\n",
            "|      AverageQVals |           -22.1 |\n",
            "|          StdQVals |            66.1 |\n",
            "|          MaxQVals |            30.2 |\n",
            "|          MinQVals |            -225 |\n",
            "|            LossPi |            20.5 |\n",
            "|             LossQ |            17.5 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              10 |\n",
            "|      AverageEpRet |            -156 |\n",
            "|          StdEpRet |            79.2 |\n",
            "|          MaxEpRet |           -1.25 |\n",
            "|          MinEpRet |            -325 |\n",
            "|  AverageTestEpRet |            -138 |\n",
            "|      StdTestEpRet |            61.7 |\n",
            "|      MaxTestEpRet |           -8.09 |\n",
            "|      MinTestEpRet |            -247 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |           4e+04 |\n",
            "|      AverageQVals |           -23.2 |\n",
            "|          StdQVals |            64.4 |\n",
            "|          MaxQVals |            23.8 |\n",
            "|          MinQVals |            -230 |\n",
            "|            LossPi |            21.8 |\n",
            "|             LossQ |            15.8 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              11 |\n",
            "|      AverageEpRet |            -133 |\n",
            "|          StdEpRet |            94.9 |\n",
            "|          MaxEpRet |           -4.67 |\n",
            "|          MinEpRet |            -352 |\n",
            "|  AverageTestEpRet |            -186 |\n",
            "|      StdTestEpRet |            58.1 |\n",
            "|      MaxTestEpRet |            -125 |\n",
            "|      MinTestEpRet |            -250 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         4.4e+04 |\n",
            "|      AverageQVals |             -23 |\n",
            "|          StdQVals |            63.3 |\n",
            "|          MaxQVals |            20.4 |\n",
            "|          MinQVals |            -232 |\n",
            "|            LossPi |            21.8 |\n",
            "|             LossQ |            17.7 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              12 |\n",
            "|      AverageEpRet |            -171 |\n",
            "|          StdEpRet |             110 |\n",
            "|          MaxEpRet |           -7.81 |\n",
            "|          MinEpRet |            -382 |\n",
            "|  AverageTestEpRet |            -163 |\n",
            "|      StdTestEpRet |            53.3 |\n",
            "|      MaxTestEpRet |            -124 |\n",
            "|      MinTestEpRet |            -261 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         4.8e+04 |\n",
            "|      AverageQVals |           -22.9 |\n",
            "|          StdQVals |              62 |\n",
            "|          MaxQVals |            17.3 |\n",
            "|          MinQVals |            -236 |\n",
            "|            LossPi |            21.8 |\n",
            "|             LossQ |            16.7 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              13 |\n",
            "|      AverageEpRet |            -170 |\n",
            "|          StdEpRet |            70.8 |\n",
            "|          MaxEpRet |            -1.4 |\n",
            "|          MinEpRet |            -290 |\n",
            "|  AverageTestEpRet |            -178 |\n",
            "|      StdTestEpRet |            62.2 |\n",
            "|      MaxTestEpRet |            -121 |\n",
            "|      MinTestEpRet |            -273 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         5.2e+04 |\n",
            "|      AverageQVals |           -23.3 |\n",
            "|          StdQVals |            61.3 |\n",
            "|          MaxQVals |            18.4 |\n",
            "|          MinQVals |            -241 |\n",
            "|            LossPi |            22.2 |\n",
            "|             LossQ |            14.2 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              14 |\n",
            "|      AverageEpRet |            -152 |\n",
            "|          StdEpRet |            78.2 |\n",
            "|          MaxEpRet |           -16.6 |\n",
            "|          MinEpRet |            -360 |\n",
            "|  AverageTestEpRet |            -171 |\n",
            "|      StdTestEpRet |            79.1 |\n",
            "|      MaxTestEpRet |           -3.98 |\n",
            "|      MinTestEpRet |            -252 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         5.6e+04 |\n",
            "|      AverageQVals |           -22.1 |\n",
            "|          StdQVals |            60.6 |\n",
            "|          MaxQVals |            19.4 |\n",
            "|          MinQVals |            -246 |\n",
            "|            LossPi |            21.1 |\n",
            "|             LossQ |              14 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              15 |\n",
            "|      AverageEpRet |            -134 |\n",
            "|          StdEpRet |            75.6 |\n",
            "|          MaxEpRet |           -1.83 |\n",
            "|          MinEpRet |            -263 |\n",
            "|  AverageTestEpRet |            -136 |\n",
            "|      StdTestEpRet |            81.7 |\n",
            "|      MaxTestEpRet |           -6.22 |\n",
            "|      MinTestEpRet |            -251 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |           6e+04 |\n",
            "|      AverageQVals |           -21.5 |\n",
            "|          StdQVals |            59.1 |\n",
            "|          MaxQVals |            15.7 |\n",
            "|          MinQVals |            -245 |\n",
            "|            LossPi |            20.5 |\n",
            "|             LossQ |            13.4 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              16 |\n",
            "|      AverageEpRet |            -136 |\n",
            "|          StdEpRet |            82.2 |\n",
            "|          MaxEpRet |           -2.25 |\n",
            "|          MinEpRet |            -346 |\n",
            "|  AverageTestEpRet |            -146 |\n",
            "|      StdTestEpRet |            72.6 |\n",
            "|      MaxTestEpRet |              -3 |\n",
            "|      MinTestEpRet |            -256 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         6.4e+04 |\n",
            "|      AverageQVals |           -21.4 |\n",
            "|          StdQVals |            57.5 |\n",
            "|          MaxQVals |            10.8 |\n",
            "|          MinQVals |            -251 |\n",
            "|            LossPi |            20.5 |\n",
            "|             LossQ |            12.5 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              17 |\n",
            "|      AverageEpRet |            -140 |\n",
            "|          StdEpRet |            83.5 |\n",
            "|          MaxEpRet |           -4.76 |\n",
            "|          MinEpRet |            -339 |\n",
            "|  AverageTestEpRet |            -179 |\n",
            "|      StdTestEpRet |            91.8 |\n",
            "|      MaxTestEpRet |            -121 |\n",
            "|      MinTestEpRet |            -358 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         6.8e+04 |\n",
            "|      AverageQVals |           -21.6 |\n",
            "|          StdQVals |            57.1 |\n",
            "|          MaxQVals |            10.1 |\n",
            "|          MinQVals |            -247 |\n",
            "|            LossPi |            20.7 |\n",
            "|             LossQ |            12.8 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              18 |\n",
            "|      AverageEpRet |            -154 |\n",
            "|          StdEpRet |            86.3 |\n",
            "|          MaxEpRet |           -1.05 |\n",
            "|          MinEpRet |            -341 |\n",
            "|  AverageTestEpRet |            -177 |\n",
            "|      StdTestEpRet |            87.6 |\n",
            "|      MaxTestEpRet |           -5.78 |\n",
            "|      MinTestEpRet |            -328 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         7.2e+04 |\n",
            "|      AverageQVals |           -21.2 |\n",
            "|          StdQVals |            56.3 |\n",
            "|          MaxQVals |            8.98 |\n",
            "|          MinQVals |            -252 |\n",
            "|            LossPi |            20.4 |\n",
            "|             LossQ |              12 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              19 |\n",
            "|      AverageEpRet |            -182 |\n",
            "|          StdEpRet |              86 |\n",
            "|          MaxEpRet |           -3.07 |\n",
            "|          MinEpRet |            -348 |\n",
            "|  AverageTestEpRet |            -145 |\n",
            "|      StdTestEpRet |             104 |\n",
            "|      MaxTestEpRet |          -0.661 |\n",
            "|      MinTestEpRet |            -358 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |         7.6e+04 |\n",
            "|      AverageQVals |           -20.9 |\n",
            "|          StdQVals |            55.2 |\n",
            "|          MaxQVals |             7.9 |\n",
            "|          MinQVals |            -252 |\n",
            "|            LossPi |            20.2 |\n",
            "|             LossQ |            11.8 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|             Epoch |              20 |\n",
            "|      AverageEpRet |            -149 |\n",
            "|          StdEpRet |            81.7 |\n",
            "|          MaxEpRet |          -0.494 |\n",
            "|          MinEpRet |            -357 |\n",
            "|  AverageTestEpRet |            -190 |\n",
            "|      StdTestEpRet |            71.1 |\n",
            "|      MaxTestEpRet |            -118 |\n",
            "|      MinTestEpRet |            -324 |\n",
            "|             EpLen |             200 |\n",
            "|         TestEpLen |             200 |\n",
            "| TotalEnvInteracts |           8e+04 |\n",
            "|      AverageQVals |           -20.9 |\n",
            "|          StdQVals |            54.8 |\n",
            "|          MaxQVals |            7.46 |\n",
            "|          MinQVals |            -255 |\n",
            "|            LossPi |            20.2 |\n",
            "|             LossQ |            10.2 |\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Prepare for interaction with environment\n",
        "total_steps = steps_per_epoch * epochs\n",
        "o, ep_ret, ep_len = env.reset(), 0, 0\n",
        "\n",
        "# Main loop: collect experience in env and update/log each epoch\n",
        "for t in range(total_steps):\n",
        "    \n",
        "    # Until start_steps have elapsed, randomly sample actions\n",
        "    # from a uniform distribution for better exploration. Afterwards, \n",
        "    # use the learned policy (with some noise, via act_noise). \n",
        "    if t > start_steps:\n",
        "        a = get_action(o, act_noise)\n",
        "    else:\n",
        "        a = env.action_space.sample()\n",
        "\n",
        "    # Step the env\n",
        "    o2, r, d, _ = env.step(a)\n",
        "    ep_ret += r\n",
        "    ep_len += 1\n",
        "\n",
        "    # Ignore the \"done\" signal if it comes from hitting the time\n",
        "    # horizon (that is, when it's an artificial terminal signal\n",
        "    # that isn't based on the agent's state)\n",
        "    d = False if ep_len==max_ep_len else d\n",
        "\n",
        "    # Store experience to replay buffer\n",
        "    replay_buffer.store(o, a, r, o2, d)\n",
        "\n",
        "    # Super critical, easy to overlook step: make sure to update \n",
        "    # most recent observation!\n",
        "    o = o2\n",
        "\n",
        "    # End of trajectory handling\n",
        "    if d or (ep_len == max_ep_len):\n",
        "        logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
        "        o, ep_ret, ep_len = env.reset(), 0, 0\n",
        "\n",
        "    # Update handling\n",
        "    if t >= update_after and t % update_every == 0:\n",
        "        for _ in range(update_every):\n",
        "            batch = replay_buffer.sample_batch(batch_size)\n",
        "            update(data=batch)\n",
        "\n",
        "    # End of epoch handling\n",
        "    if (t+1) % steps_per_epoch == 0:\n",
        "        epoch = (t+1) // steps_per_epoch\n",
        "\n",
        "        # Save model\n",
        "        if (epoch % save_freq == 0) or (epoch == epochs):\n",
        "            logger.save_state({'env': env}, None)\n",
        "\n",
        "        # Test the performance of the deterministic version of the agent.\n",
        "        test_agent()\n",
        "\n",
        "        # Log info about epoch\n",
        "        logger.log_tabular('Epoch', epoch)\n",
        "        logger.log_tabular('EpRet', with_min_and_max=True)\n",
        "        logger.log_tabular('TestEpRet', with_min_and_max=True)\n",
        "        logger.log_tabular('EpLen', average_only=True)\n",
        "        logger.log_tabular('TestEpLen', average_only=True)\n",
        "        logger.log_tabular('TotalEnvInteracts', t)\n",
        "        logger.log_tabular('QVals', with_min_and_max=True)\n",
        "        logger.log_tabular('LossPi', average_only=True)\n",
        "        logger.log_tabular('LossQ', average_only=True)\n",
        "        logger.dump_tabular()\n",
        "\n",
        "env.close()\n",
        "test_env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OLK8id8TShS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "df69f7b5-e606-4901-baed-73438c7f0bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No file named config.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure()\n",
        "data = get_datasets(ddpg_output_dir)\n",
        "plot_data(data, xaxis='TotalEnvInteracts', value='Performance', smooth=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9iyCfx7TShS"
      },
      "source": [
        "We can also run the trained agents as below (set render=True to visualize - note that additional settings may be required to view on Colab or on a remote server)\n",
        "\n",
        "Note: In order for this code to work, the MLPActorCritic model definition (and dependencies) should be in scope.\n",
        "\n",
        "A solved pendulum controller would typically have EpRet > -150."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EfZnc4HTShT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a88e0fdd-b748-40b9-d277-1c8edf031a43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Loading from /content/drive/MyDrive/Users/青椒/CS523/hw5_stuffs/ddpg/pyt_save/model.pt.\n",
            "\n",
            "\n",
            "\u001b[32;1mLogging data to /tmp/experiments/1650469895/progress.txt\u001b[0m\n",
            "Episode 0 \t EpRet -1.371 \t EpLen 200\n",
            "Episode 1 \t EpRet -116.307 \t EpLen 200\n",
            "Episode 2 \t EpRet -117.677 \t EpLen 200\n",
            "Episode 3 \t EpRet -125.480 \t EpLen 200\n",
            "Episode 4 \t EpRet -118.403 \t EpLen 200\n",
            "Episode 5 \t EpRet -120.313 \t EpLen 200\n",
            "Episode 6 \t EpRet -261.474 \t EpLen 200\n",
            "Episode 7 \t EpRet -124.682 \t EpLen 200\n",
            "Episode 8 \t EpRet -121.751 \t EpLen 200\n",
            "Episode 9 \t EpRet -126.205 \t EpLen 200\n",
            "-------------------------------------\n",
            "|    AverageEpRet |            -123 |\n",
            "|        StdEpRet |            58.4 |\n",
            "|        MaxEpRet |           -1.37 |\n",
            "|        MinEpRet |            -261 |\n",
            "|           EpLen |             200 |\n",
            "-------------------------------------\n"
          ]
        }
      ],
      "source": [
        "playback_env, get_action = load_policy_and_env(ddpg_output_dir, 'last', True)\n",
        "run_policy(playback_env, get_action, num_episodes=10, render=False)\n",
        "playback_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tMPpAqRpR2N"
      },
      "source": [
        "## Q2 Sequence to Sequence Modelling with nn.Transformer and Torch Text (20 points)\n",
        "\n",
        "You will implement a part of transformer. This question aims to let you to get familiar with the transformer architecture purposed in the paper [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf). This question is modified from the original pytorch tutorial [here](https://pytorch.org/tutorials/beginner/transformer_tutorial.html?highlight=transformer), you can refer it when you fill out the code. The general architecture of trasnsformer is shown in the figure below:\n",
        "\n",
        "<img src=\"http://ai.bu.edu/DL523/HW5_files/transformer_architecture.jpg\" width=\"360em\">\n",
        "\n",
        "This question requires you to implement a sequence to sequence model by encoder, which is the left part of the figure. You will use integrated layers in pytorch.\n",
        "\n",
        "The transformer model has been proved to be superior in quality for many sequence-to-sequence\n",
        "problems while being more parallelizable. The ``nn.Transformer`` module\n",
        "relies entirely on an attention mechanism (another module recently\n",
        "implemented as `nn.MultiheadAttention`) to draw global dependencies\n",
        "between input and output. The ``nn.Transformer`` module is now highly\n",
        "modularized such that a single component (like [`nn.TransformerEncoder `](<https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder>)\n",
        "in this tutorial) can be easily adapted/composed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFKFpMoJd0mu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkeaGn8INY9k"
      },
      "source": [
        "### Q2.1 Define the model \n",
        "In this question, we train ``nn.TransformerEncoder`` model on a\n",
        "language modeling task. The language modeling task is to assign a\n",
        "probability for the likelihood of a given word (or a sequence of words)\n",
        "to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
        "layer first, followed by a positional encoding layer to account for the order\n",
        "of the word (see the next paragraph for more details). The\n",
        "``nn.TransformerEncoder`` consists of multiple layers of\n",
        "``nn.TransformerEncoderLayer`` . Along with the input sequence, a square\n",
        "attention mask is required because the self-attention layers in\n",
        "``nn.TransformerEncoder`` are only allowed to attend the earlier positions in\n",
        "the sequence. For the language modeling task, any tokens on the future\n",
        "positions should be masked. To have the actual words, the output\n",
        "of ``nn.TransformerEncoder`` model is sent to the final Linear\n",
        "layer, which is followed by a log-Softmax function. We will see how to implement the ``PositionalEncoding`` in the later question. \n",
        "\n",
        "<img src=\"http://ai.bu.edu/DL523/HW5_files/encoder.png\" width=\"em\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHI3LBIcgGVO"
      },
      "source": [
        "In the following model, we only train a encoder model, which is the left part of the figure. Then we concatenate a Linear model `self.decoder` to replace the right part of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzlf5iVYTShU"
      },
      "source": [
        "**TO AVOID ISSUES WITH PACKAGE IMPORTS, IT MIGHT BE WORTH RESTARTING THE NOTEBOOK KERNEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ai9dTxjUNS5-"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import typing\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 token_nums: int,\n",
        "                 input_embedding_dim: int,\n",
        "                 multi_head_nums: int,\n",
        "                 hidden_nums: int,\n",
        "                 layer_nums: int,\n",
        "                 dropout: float = 0.5):\n",
        "        \"\"\"This is a transformer encoder model, the input arguments are as follows:\n",
        "\n",
        "        :param token_nums dimension of tokens\n",
        "        :param input_embedding_dim dimension of input embeddings\n",
        "        :param multi_head_nums the number of heads in the Multi-Head Attention model\n",
        "        :param hidden_nums dimension of the hidden encoding between two layers of TransformerEncoderLayer\n",
        "        :param layer_nums number of TransformerEncoderLayer layers\n",
        "        \"\"\"\n",
        "        super(TransformerModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "        self.model_type = 'Transformer'\n",
        "\n",
        "        self.embedding_encoder = nn.Embedding(token_nums, input_embedding_dim)\n",
        "        self.input_embedding_dim = input_embedding_dim\n",
        "\n",
        "        self.positional_encoder = PositionalEncoding(input_embedding_dim, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(input_embedding_dim, multi_head_nums, hidden_nums, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, layer_nums)\n",
        "        self.linear_decoder = nn.Linear(input_embedding_dim, token_nums)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def generate_square_subsequent_mask(self, size: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Generate a square mask for the sequence.\n",
        "        The masked positions are filled with float('-inf').\n",
        "        :param size: the size of the mask\n",
        "        :return: A series of masked vectors.\n",
        "        \"\"\"\n",
        "\n",
        "        # actually returns a matrix of size * size, with -inf in the first row\n",
        "        # except for the first element, which is 0. Then on the next lines, more\n",
        "        # and more elements that are not -inf are added,\n",
        "        # until the last line, when there are no more -inf elements\n",
        "        #\n",
        "        # tensor([[0., -inf, -inf, -inf, -inf],\n",
        "        #         [0.,   0., -inf, -inf, -inf],\n",
        "        #         [0.,   0.,   0., -inf, -inf],\n",
        "        #         [0.,   0.,   0.,   0., -inf],\n",
        "        #         [0.,   0.,   0.,   0.,   0.]])\n",
        "\n",
        "        mask = torch.triu(torch.ones(size, size) * float('-inf') , diagonal=1)\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        init_range = 0.1\n",
        "        self.embedding_encoder.weight.data.uniform_(-init_range, init_range)\n",
        "        self.linear_decoder.bias.data.zero_()\n",
        "        self.linear_decoder.weight.data.uniform_(-init_range, init_range)\n",
        "\n",
        "    def forward(self,\n",
        "                src: torch.Tensor,\n",
        "                src_mask: torch.Tensor) -> torch.Tensor:\n",
        "        # 1. We will convert tokens into embeddings\n",
        "        src = self.embedding_encoder(src) * math.sqrt(self.input_embedding_dim)\n",
        "\n",
        "        # 2. We will add positional encoding\n",
        "        src = self.positional_encoder(src)\n",
        "\n",
        "        # 3. We will apply the TransformerEncoder\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "\n",
        "        # 4. We will apply a linear layer to the output\n",
        "        output = self.linear_decoder(output)\n",
        "        \n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-BxUnw6MZf5"
      },
      "source": [
        "### Q2.2 Positional Encoding\n",
        "#### Q2.2.1 Fill the code block\n",
        "``PositionalEncoding`` module injects some information about the\n",
        "relative or absolute position of the tokens in the sequence. The\n",
        "positional encodings have the same dimension as the embeddings so that\n",
        "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
        "different frequencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6A0pUKNMpQ84"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements the positional encoding as described in \"Attention is all you need\"\n",
        "\n",
        "    `PositionalEncoding` module injects some information about the relative or absolute position of the tokens in the\n",
        "    sequence.\n",
        "\n",
        "    The positional encodings have the same dimension as the embeddings so that the two can be summed. Here,\n",
        "    we use sine and cosine functions of different frequencies.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 embedding_dim: int,\n",
        "                 dropout: int =0.1,\n",
        "                 max_length: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_length).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(10000.0) / embedding_dim))\n",
        "        pe = torch.zeros(max_length, 1, embedding_dim)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"YOUR CODE HERE\"\"\"\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        output = self.dropout(x)\n",
        "        \"\"\"YOUR CODE ENDS\"\"\"\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7mD6P7eCaKs"
      },
      "source": [
        "#### Q2.2.2 Why do we need this positional encoding in the transformer architecture?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtV4WUO5TShZ"
      },
      "source": [
        "Positional encoding provides a solution that helps to account for the order of words in the input sequence. It will introduce a new vector values to the embedding, which contains information about the meaningful distances between word's positions and distances. This can helps the model to recover the position of each words during output stage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i82IJ2chNstR"
      },
      "source": [
        "### Q2.3 Running the model\n",
        "\n",
        "#### Q2.3.1 Run the code to get desired performance.\n",
        "The training process uses Wikitext-2 dataset from ``torchtext``. The\n",
        "vocab object is built based on the train dataset and is used to numericalize\n",
        "tokens into tensors. Starting from sequential data, the ``batchify()``\n",
        "function arranges the dataset into columns, trimming off any tokens remaining\n",
        "after the data has been divided into batches of size ``batch_size``.\n",
        "For instance, with the alphabet as the sequence (total length of 26)\n",
        "and a batch size of 4, we would divide the alphabet into 4 sequences of\n",
        "length 6:\n",
        "\n",
        "$$\n",
        "\\begin{align}\\begin{bmatrix}\n",
        "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
        "  \\end{bmatrix}\n",
        "  \\Rightarrow\n",
        "  \\begin{bmatrix}\n",
        "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
        "  \\end{bmatrix}\\end{align}\n",
        "$$\n",
        "\n",
        "These columns are treated as independent by the model, which means that\n",
        "the dependence of ``G`` and ``F`` can not be learned, but allows more\n",
        "efficient batch processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5xSh4VITShZ"
      },
      "source": [
        "**Note**, we're using torchtext v 0.11.0\n",
        "\n",
        "You *may* need to run the following code block below if running locally if errors are thrown about missing packages or components (though check the rest first before going through this)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSd6iKWwTShZ"
      },
      "outputs": [],
      "source": [
        "!pip install \"torchtext==0.11\"\n",
        "!pip install \"spacy>=2.2.4,<=3.2.4\"\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv4yr-K3OHSs"
      },
      "outputs": [],
      "source": [
        "import torchtext\n",
        "from torchtext.legacy import data\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "TEXT = torchtext.legacy.data.Field(tokenize=get_tokenizer(\"spacy\"),\n",
        "                                   init_token='<sos>',\n",
        "                                   eos_token='<eos>',\n",
        "                                   lower=True)\n",
        "\n",
        "train_txt, val_txt, test_txt = torchtext.legacy.datasets.WikiText2.splits(TEXT)\n",
        "TEXT.build_vocab(train_txt)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def batchify(data: Dataset,\n",
        "             batch_size: int) -> torch.Tensor:\n",
        "    data = TEXT.numericalize([data.examples[0].text])\n",
        "    # Divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // batch_size\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * batch_size)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_txt, batch_size)\n",
        "val_data = batchify(val_txt, eval_batch_size)\n",
        "test_data = batchify(test_txt, eval_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjC_fNXWOcIJ"
      },
      "source": [
        "The ``get_batch()`` function generates the input and target sequence for\n",
        "the transformer model. It subdivides the source data into chunks of\n",
        "length ``bptt``. For the language modeling task, the model needs the\n",
        "following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
        "we’d get the following two Variables for ``i`` = 0:\n",
        "\n",
        "<img src=\"http://ai.bu.edu/DL523/HW5_files/transformer_input_target1.png\" width=\"em\">\n",
        "<!-- ![](transformer_input_target.png) -->\n",
        "\n",
        "\n",
        "It should be noted that the chunks are along dimension 0, consistent\n",
        "with the ``S`` dimension in the Transformer model. The batch dimension\n",
        "``N`` is along dimension 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxe4vOD8Oh7S"
      },
      "outputs": [],
      "source": [
        "# The get_batch() function will use the value bptt to subdivide the data into chunks of length bptt.\n",
        "# For example, if bptt is 2, we get the following two variables with i = 0.\n",
        "bptt: int = 35\n",
        "\n",
        "\n",
        "def get_batch(source: torch.Tensor, i: int) -> typing.Tuple[torch.Tensor, torch.Tensor]:\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i + seq_len]\n",
        "    target = source[i + 1:i + 1 + seq_len].reshape(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "ntokens = len(TEXT.vocab.stoi)  # the size of vocabulary\n",
        "emsize = 200  # embedding dimension\n",
        "nhid = 200  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # the number of heads in the multiheadattention models\n",
        "dropout = 0.2  # the dropout value\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ss9NoUZY0Rz"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "def train() -> None:\n",
        "    model.train()  # Turn on the train mode\n",
        "    total_loss = 0.0\n",
        "    log_interval = 10\n",
        "    start_time = time.time()\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    num_batches = len(train_data) // bptt\n",
        "\n",
        "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
        "    print(\"in training loop\")\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        batch_size = data.size(0)\n",
        "\n",
        "        if batch_size != bptt:\n",
        "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
        "        output = model(data, src_mask)\n",
        "\n",
        "        # begin to calculate the loss\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            # because the learning rate is changing,\n",
        "            # we need to fetch the latest learning rate.\n",
        "            learningRate = scheduler.get_last_lr()[0]\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {learningRate:02.2f} | ms/batch {elapsed * 1000 / log_interval:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {math.exp(cur_loss):8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: torch.Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            batch_size = data.size(0)\n",
        "            if batch_size != bptt:\n",
        "                src_mask = src_mask[:batch_size, :batch_size]\n",
        "            output = model(data, src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5niRh2AI6qeN"
      },
      "source": [
        "Running the code block below. You will get around 220 ppl on training at the end of epoch 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CB2g1K5ZDBf",
        "outputId": "0e6906f9-6959-4833-d2d6-fd401222abd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "starting\n",
            "in training loop\n",
            "| epoch   1 |    10/ 3195 batches | lr 5.00 | ms/batch 32.10 | loss 10.90 | ppl 54110.16\n",
            "| epoch   1 |    20/ 3195 batches | lr 5.00 | ms/batch  7.60 | loss  8.65 | ppl  5702.83\n",
            "| epoch   1 |    30/ 3195 batches | lr 5.00 | ms/batch  7.43 | loss  8.67 | ppl  5827.13\n",
            "| epoch   1 |    40/ 3195 batches | lr 5.00 | ms/batch  7.55 | loss  7.98 | ppl  2919.76\n",
            "| epoch   1 |    50/ 3195 batches | lr 5.00 | ms/batch  8.22 | loss  7.60 | ppl  1990.59\n",
            "| epoch   1 |    60/ 3195 batches | lr 5.00 | ms/batch  7.45 | loss  7.47 | ppl  1752.93\n",
            "| epoch   1 |    70/ 3195 batches | lr 5.00 | ms/batch  7.34 | loss  7.21 | ppl  1355.31\n",
            "| epoch   1 |    80/ 3195 batches | lr 5.00 | ms/batch  7.29 | loss  7.18 | ppl  1315.49\n",
            "| epoch   1 |    90/ 3195 batches | lr 5.00 | ms/batch  7.21 | loss  6.93 | ppl  1024.09\n",
            "| epoch   1 |   100/ 3195 batches | lr 5.00 | ms/batch  7.57 | loss  7.04 | ppl  1144.61\n",
            "| epoch   1 |   110/ 3195 batches | lr 5.00 | ms/batch  7.03 | loss  6.80 | ppl   897.44\n",
            "| epoch   1 |   120/ 3195 batches | lr 5.00 | ms/batch  6.98 | loss  6.85 | ppl   939.93\n",
            "| epoch   1 |   130/ 3195 batches | lr 5.00 | ms/batch  7.53 | loss  6.89 | ppl   980.45\n",
            "| epoch   1 |   140/ 3195 batches | lr 5.00 | ms/batch  7.05 | loss  6.83 | ppl   928.31\n",
            "| epoch   1 |   150/ 3195 batches | lr 5.00 | ms/batch  7.21 | loss  6.67 | ppl   786.32\n",
            "| epoch   1 |   160/ 3195 batches | lr 5.00 | ms/batch  7.25 | loss  6.71 | ppl   816.64\n",
            "| epoch   1 |   170/ 3195 batches | lr 5.00 | ms/batch  7.24 | loss  6.50 | ppl   662.25\n",
            "| epoch   1 |   180/ 3195 batches | lr 5.00 | ms/batch  7.34 | loss  6.56 | ppl   706.45\n",
            "| epoch   1 |   190/ 3195 batches | lr 5.00 | ms/batch  7.60 | loss  6.59 | ppl   725.62\n",
            "| epoch   1 |   200/ 3195 batches | lr 5.00 | ms/batch  7.33 | loss  6.55 | ppl   698.39\n",
            "| epoch   1 |   210/ 3195 batches | lr 5.00 | ms/batch  7.29 | loss  6.76 | ppl   862.56\n",
            "| epoch   1 |   220/ 3195 batches | lr 5.00 | ms/batch  7.28 | loss  6.64 | ppl   767.13\n",
            "| epoch   1 |   230/ 3195 batches | lr 5.00 | ms/batch  6.90 | loss  6.36 | ppl   576.85\n",
            "| epoch   1 |   240/ 3195 batches | lr 5.00 | ms/batch  6.96 | loss  6.40 | ppl   599.46\n",
            "| epoch   1 |   250/ 3195 batches | lr 5.00 | ms/batch  7.71 | loss  6.32 | ppl   553.61\n",
            "| epoch   1 |   260/ 3195 batches | lr 5.00 | ms/batch  6.97 | loss  6.35 | ppl   572.41\n",
            "| epoch   1 |   270/ 3195 batches | lr 5.00 | ms/batch  7.19 | loss  6.33 | ppl   561.55\n",
            "| epoch   1 |   280/ 3195 batches | lr 5.00 | ms/batch  7.65 | loss  6.21 | ppl   495.48\n",
            "| epoch   1 |   290/ 3195 batches | lr 5.00 | ms/batch  7.26 | loss  6.40 | ppl   599.67\n",
            "| epoch   1 |   300/ 3195 batches | lr 5.00 | ms/batch  7.11 | loss  6.43 | ppl   618.11\n",
            "| epoch   1 |   310/ 3195 batches | lr 5.00 | ms/batch  7.42 | loss  6.31 | ppl   550.16\n",
            "| epoch   1 |   320/ 3195 batches | lr 5.00 | ms/batch  7.27 | loss  6.18 | ppl   482.66\n",
            "| epoch   1 |   330/ 3195 batches | lr 5.00 | ms/batch  7.53 | loss  6.25 | ppl   519.15\n",
            "| epoch   1 |   340/ 3195 batches | lr 5.00 | ms/batch  7.77 | loss  6.08 | ppl   438.78\n",
            "| epoch   1 |   350/ 3195 batches | lr 5.00 | ms/batch  7.36 | loss  6.02 | ppl   410.24\n",
            "| epoch   1 |   360/ 3195 batches | lr 5.00 | ms/batch  6.99 | loss  6.17 | ppl   478.63\n",
            "| epoch   1 |   370/ 3195 batches | lr 5.00 | ms/batch  7.17 | loss  6.04 | ppl   420.49\n",
            "| epoch   1 |   380/ 3195 batches | lr 5.00 | ms/batch  6.99 | loss  6.17 | ppl   475.80\n",
            "| epoch   1 |   390/ 3195 batches | lr 5.00 | ms/batch  7.08 | loss  6.06 | ppl   427.10\n",
            "| epoch   1 |   400/ 3195 batches | lr 5.00 | ms/batch  7.24 | loss  6.04 | ppl   419.92\n",
            "| epoch   1 |   410/ 3195 batches | lr 5.00 | ms/batch  7.03 | loss  6.21 | ppl   499.68\n",
            "| epoch   1 |   420/ 3195 batches | lr 5.00 | ms/batch  7.01 | loss  6.07 | ppl   431.44\n",
            "| epoch   1 |   430/ 3195 batches | lr 5.00 | ms/batch  7.40 | loss  5.98 | ppl   395.26\n",
            "| epoch   1 |   440/ 3195 batches | lr 5.00 | ms/batch  7.13 | loss  6.07 | ppl   433.49\n",
            "| epoch   1 |   450/ 3195 batches | lr 5.00 | ms/batch  7.02 | loss  5.96 | ppl   389.20\n",
            "| epoch   1 |   460/ 3195 batches | lr 5.00 | ms/batch  7.64 | loss  5.84 | ppl   342.19\n",
            "| epoch   1 |   470/ 3195 batches | lr 5.00 | ms/batch  7.36 | loss  6.01 | ppl   406.57\n",
            "| epoch   1 |   480/ 3195 batches | lr 5.00 | ms/batch  7.65 | loss  5.91 | ppl   369.68\n",
            "| epoch   1 |   490/ 3195 batches | lr 5.00 | ms/batch  7.28 | loss  6.11 | ppl   451.01\n",
            "| epoch   1 |   500/ 3195 batches | lr 5.00 | ms/batch  7.07 | loss  5.82 | ppl   338.59\n",
            "| epoch   1 |   510/ 3195 batches | lr 5.00 | ms/batch  7.16 | loss  5.94 | ppl   381.70\n",
            "| epoch   1 |   520/ 3195 batches | lr 5.00 | ms/batch  7.27 | loss  6.06 | ppl   427.27\n",
            "| epoch   1 |   530/ 3195 batches | lr 5.00 | ms/batch  7.02 | loss  6.03 | ppl   413.84\n",
            "| epoch   1 |   540/ 3195 batches | lr 5.00 | ms/batch  7.13 | loss  5.94 | ppl   378.27\n",
            "| epoch   1 |   550/ 3195 batches | lr 5.00 | ms/batch  7.32 | loss  5.93 | ppl   374.45\n",
            "| epoch   1 |   560/ 3195 batches | lr 5.00 | ms/batch  7.33 | loss  5.87 | ppl   353.14\n",
            "| epoch   1 |   570/ 3195 batches | lr 5.00 | ms/batch  7.02 | loss  5.92 | ppl   373.26\n",
            "| epoch   1 |   580/ 3195 batches | lr 5.00 | ms/batch  7.50 | loss  5.88 | ppl   358.55\n",
            "| epoch   1 |   590/ 3195 batches | lr 5.00 | ms/batch  7.23 | loss  5.84 | ppl   342.38\n",
            "| epoch   1 |   600/ 3195 batches | lr 5.00 | ms/batch  7.15 | loss  5.79 | ppl   326.66\n",
            "| epoch   1 |   610/ 3195 batches | lr 5.00 | ms/batch  7.33 | loss  5.79 | ppl   327.16\n",
            "| epoch   1 |   620/ 3195 batches | lr 5.00 | ms/batch  7.78 | loss  5.88 | ppl   358.51\n",
            "| epoch   1 |   630/ 3195 batches | lr 5.00 | ms/batch  6.96 | loss  5.78 | ppl   324.46\n",
            "| epoch   1 |   640/ 3195 batches | lr 5.00 | ms/batch  7.17 | loss  5.67 | ppl   290.61\n",
            "| epoch   1 |   650/ 3195 batches | lr 5.00 | ms/batch  6.98 | loss  6.01 | ppl   406.27\n",
            "| epoch   1 |   660/ 3195 batches | lr 5.00 | ms/batch  7.07 | loss  5.83 | ppl   340.37\n",
            "| epoch   1 |   670/ 3195 batches | lr 5.00 | ms/batch  7.11 | loss  5.79 | ppl   327.20\n",
            "| epoch   1 |   680/ 3195 batches | lr 5.00 | ms/batch  7.13 | loss  5.78 | ppl   324.12\n",
            "| epoch   1 |   690/ 3195 batches | lr 5.00 | ms/batch  6.92 | loss  5.69 | ppl   296.12\n",
            "| epoch   1 |   700/ 3195 batches | lr 5.00 | ms/batch  7.67 | loss  5.92 | ppl   371.81\n",
            "| epoch   1 |   710/ 3195 batches | lr 5.00 | ms/batch  6.96 | loss  5.89 | ppl   360.54\n",
            "| epoch   1 |   720/ 3195 batches | lr 5.00 | ms/batch  7.03 | loss  5.79 | ppl   327.30\n",
            "| epoch   1 |   730/ 3195 batches | lr 5.00 | ms/batch  7.39 | loss  5.83 | ppl   339.07\n",
            "| epoch   1 |   740/ 3195 batches | lr 5.00 | ms/batch  7.32 | loss  5.74 | ppl   310.75\n",
            "| epoch   1 |   750/ 3195 batches | lr 5.00 | ms/batch  7.04 | loss  5.77 | ppl   322.14\n",
            "| epoch   1 |   760/ 3195 batches | lr 5.00 | ms/batch  8.04 | loss  5.66 | ppl   288.55\n",
            "| epoch   1 |   770/ 3195 batches | lr 5.00 | ms/batch  6.97 | loss  5.64 | ppl   280.42\n",
            "| epoch   1 |   780/ 3195 batches | lr 5.00 | ms/batch  7.16 | loss  5.76 | ppl   318.92\n",
            "| epoch   1 |   790/ 3195 batches | lr 5.00 | ms/batch  7.25 | loss  5.75 | ppl   314.65\n",
            "| epoch   1 |   800/ 3195 batches | lr 5.00 | ms/batch  6.88 | loss  5.61 | ppl   273.51\n",
            "| epoch   1 |   810/ 3195 batches | lr 5.00 | ms/batch  7.22 | loss  5.93 | ppl   375.19\n",
            "| epoch   1 |   820/ 3195 batches | lr 5.00 | ms/batch  7.44 | loss  5.75 | ppl   315.04\n",
            "| epoch   1 |   830/ 3195 batches | lr 5.00 | ms/batch  6.99 | loss  5.67 | ppl   288.65\n",
            "| epoch   1 |   840/ 3195 batches | lr 5.00 | ms/batch  6.92 | loss  5.96 | ppl   388.68\n",
            "| epoch   1 |   850/ 3195 batches | lr 5.00 | ms/batch  7.16 | loss  5.81 | ppl   335.05\n",
            "| epoch   1 |   860/ 3195 batches | lr 5.00 | ms/batch  7.33 | loss  5.82 | ppl   335.63\n",
            "| epoch   1 |   870/ 3195 batches | lr 5.00 | ms/batch  7.38 | loss  5.75 | ppl   314.44\n",
            "| epoch   1 |   880/ 3195 batches | lr 5.00 | ms/batch  7.25 | loss  5.75 | ppl   313.40\n",
            "| epoch   1 |   890/ 3195 batches | lr 5.00 | ms/batch  7.03 | loss  5.86 | ppl   349.47\n",
            "| epoch   1 |   900/ 3195 batches | lr 5.00 | ms/batch  7.67 | loss  5.87 | ppl   353.79\n",
            "| epoch   1 |   910/ 3195 batches | lr 5.00 | ms/batch  7.19 | loss  5.59 | ppl   269.04\n",
            "| epoch   1 |   920/ 3195 batches | lr 5.00 | ms/batch  6.94 | loss  5.84 | ppl   343.98\n",
            "| epoch   1 |   930/ 3195 batches | lr 5.00 | ms/batch  6.96 | loss  5.80 | ppl   331.05\n",
            "| epoch   1 |   940/ 3195 batches | lr 5.00 | ms/batch  7.14 | loss  5.89 | ppl   361.92\n",
            "| epoch   1 |   950/ 3195 batches | lr 5.00 | ms/batch  6.89 | loss  5.86 | ppl   352.31\n",
            "| epoch   1 |   960/ 3195 batches | lr 5.00 | ms/batch  6.98 | loss  5.69 | ppl   294.50\n",
            "| epoch   1 |   970/ 3195 batches | lr 5.00 | ms/batch  7.22 | loss  5.83 | ppl   339.19\n",
            "| epoch   1 |   980/ 3195 batches | lr 5.00 | ms/batch  7.44 | loss  5.69 | ppl   295.82\n",
            "| epoch   1 |   990/ 3195 batches | lr 5.00 | ms/batch  7.04 | loss  5.89 | ppl   360.09\n",
            "| epoch   1 |  1000/ 3195 batches | lr 5.00 | ms/batch  7.36 | loss  5.75 | ppl   314.29\n",
            "| epoch   1 |  1010/ 3195 batches | lr 5.00 | ms/batch  7.20 | loss  5.89 | ppl   363.15\n",
            "| epoch   1 |  1020/ 3195 batches | lr 5.00 | ms/batch  7.29 | loss  5.84 | ppl   343.46\n",
            "| epoch   1 |  1030/ 3195 batches | lr 5.00 | ms/batch  7.29 | loss  5.87 | ppl   353.48\n",
            "| epoch   1 |  1040/ 3195 batches | lr 5.00 | ms/batch  7.60 | loss  5.77 | ppl   320.48\n",
            "| epoch   1 |  1050/ 3195 batches | lr 5.00 | ms/batch  7.92 | loss  5.71 | ppl   302.30\n",
            "| epoch   1 |  1060/ 3195 batches | lr 5.00 | ms/batch  7.45 | loss  5.85 | ppl   347.13\n",
            "| epoch   1 |  1070/ 3195 batches | lr 5.00 | ms/batch  6.94 | loss  5.80 | ppl   330.79\n",
            "| epoch   1 |  1080/ 3195 batches | lr 5.00 | ms/batch  6.95 | loss  5.82 | ppl   336.27\n",
            "| epoch   1 |  1090/ 3195 batches | lr 5.00 | ms/batch  7.20 | loss  5.86 | ppl   351.01\n",
            "| epoch   1 |  1100/ 3195 batches | lr 5.00 | ms/batch  7.35 | loss  5.79 | ppl   327.41\n",
            "| epoch   1 |  1110/ 3195 batches | lr 5.00 | ms/batch  7.01 | loss  5.68 | ppl   292.18\n",
            "| epoch   1 |  1120/ 3195 batches | lr 5.00 | ms/batch  7.24 | loss  5.74 | ppl   310.39\n",
            "| epoch   1 |  1130/ 3195 batches | lr 5.00 | ms/batch  7.23 | loss  5.51 | ppl   247.70\n",
            "| epoch   1 |  1140/ 3195 batches | lr 5.00 | ms/batch  7.03 | loss  5.69 | ppl   295.62\n",
            "| epoch   1 |  1150/ 3195 batches | lr 5.00 | ms/batch  7.43 | loss  5.73 | ppl   308.29\n",
            "| epoch   1 |  1160/ 3195 batches | lr 5.00 | ms/batch  7.19 | loss  5.70 | ppl   297.50\n",
            "| epoch   1 |  1170/ 3195 batches | lr 5.00 | ms/batch  7.02 | loss  5.60 | ppl   270.38\n",
            "| epoch   1 |  1180/ 3195 batches | lr 5.00 | ms/batch  7.92 | loss  5.69 | ppl   294.50\n",
            "| epoch   1 |  1190/ 3195 batches | lr 5.00 | ms/batch  6.98 | loss  5.61 | ppl   273.45\n",
            "| epoch   1 |  1200/ 3195 batches | lr 5.00 | ms/batch  6.92 | loss  5.68 | ppl   293.86\n",
            "| epoch   1 |  1210/ 3195 batches | lr 5.00 | ms/batch  7.58 | loss  5.74 | ppl   312.43\n",
            "| epoch   1 |  1220/ 3195 batches | lr 5.00 | ms/batch  7.48 | loss  5.71 | ppl   301.42\n",
            "| epoch   1 |  1230/ 3195 batches | lr 5.00 | ms/batch  7.02 | loss  5.68 | ppl   294.39\n",
            "| epoch   1 |  1240/ 3195 batches | lr 5.00 | ms/batch  7.40 | loss  5.82 | ppl   337.58\n",
            "| epoch   1 |  1250/ 3195 batches | lr 5.00 | ms/batch  6.99 | loss  5.82 | ppl   336.50\n",
            "| epoch   1 |  1260/ 3195 batches | lr 5.00 | ms/batch  7.24 | loss  5.73 | ppl   308.59\n",
            "| epoch   1 |  1270/ 3195 batches | lr 5.00 | ms/batch  7.30 | loss  5.64 | ppl   281.39\n",
            "| epoch   1 |  1280/ 3195 batches | lr 5.00 | ms/batch  7.06 | loss  5.67 | ppl   290.15\n",
            "| epoch   1 |  1290/ 3195 batches | lr 5.00 | ms/batch  7.29 | loss  5.63 | ppl   278.42\n",
            "| epoch   1 |  1300/ 3195 batches | lr 5.00 | ms/batch  7.29 | loss  5.60 | ppl   269.23\n",
            "| epoch   1 |  1310/ 3195 batches | lr 5.00 | ms/batch  7.00 | loss  5.85 | ppl   348.68\n",
            "| epoch   1 |  1320/ 3195 batches | lr 5.00 | ms/batch  7.57 | loss  5.73 | ppl   306.91\n",
            "| epoch   1 |  1330/ 3195 batches | lr 5.00 | ms/batch  7.56 | loss  5.85 | ppl   347.90\n",
            "| epoch   1 |  1340/ 3195 batches | lr 5.00 | ms/batch  6.92 | loss  5.66 | ppl   285.91\n",
            "| epoch   1 |  1350/ 3195 batches | lr 5.00 | ms/batch  7.00 | loss  5.66 | ppl   286.62\n",
            "| epoch   1 |  1360/ 3195 batches | lr 5.00 | ms/batch  7.12 | loss  5.59 | ppl   268.41\n",
            "| epoch   1 |  1370/ 3195 batches | lr 5.00 | ms/batch  7.06 | loss  5.82 | ppl   337.14\n",
            "| epoch   1 |  1380/ 3195 batches | lr 5.00 | ms/batch  6.90 | loss  5.50 | ppl   243.73\n",
            "| epoch   1 |  1390/ 3195 batches | lr 5.00 | ms/batch  7.37 | loss  5.72 | ppl   304.83\n",
            "| epoch   1 |  1400/ 3195 batches | lr 5.00 | ms/batch  6.94 | loss  5.54 | ppl   253.90\n",
            "| epoch   1 |  1410/ 3195 batches | lr 5.00 | ms/batch  7.13 | loss  5.57 | ppl   263.10\n",
            "| epoch   1 |  1420/ 3195 batches | lr 5.00 | ms/batch  7.32 | loss  5.61 | ppl   273.98\n",
            "| epoch   1 |  1430/ 3195 batches | lr 5.00 | ms/batch  6.96 | loss  5.49 | ppl   242.77\n",
            "| epoch   1 |  1440/ 3195 batches | lr 5.00 | ms/batch  7.09 | loss  5.56 | ppl   259.77\n",
            "| epoch   1 |  1450/ 3195 batches | lr 5.00 | ms/batch  7.39 | loss  5.50 | ppl   243.64\n",
            "| epoch   1 |  1460/ 3195 batches | lr 5.00 | ms/batch  7.40 | loss  5.45 | ppl   231.84\n",
            "| epoch   1 |  1470/ 3195 batches | lr 5.00 | ms/batch  6.93 | loss  5.59 | ppl   267.90\n",
            "| epoch   1 |  1480/ 3195 batches | lr 5.00 | ms/batch  7.26 | loss  5.35 | ppl   209.99\n",
            "| epoch   1 |  1490/ 3195 batches | lr 5.00 | ms/batch  7.13 | loss  5.63 | ppl   279.80\n",
            "| epoch   1 |  1500/ 3195 batches | lr 5.00 | ms/batch  6.91 | loss  5.79 | ppl   325.55\n",
            "| epoch   1 |  1510/ 3195 batches | lr 5.00 | ms/batch  7.29 | loss  5.66 | ppl   286.38\n",
            "| epoch   1 |  1520/ 3195 batches | lr 5.00 | ms/batch  7.30 | loss  5.68 | ppl   292.39\n",
            "| epoch   1 |  1530/ 3195 batches | lr 5.00 | ms/batch  7.05 | loss  5.58 | ppl   263.77\n",
            "| epoch   1 |  1540/ 3195 batches | lr 5.00 | ms/batch  7.22 | loss  5.67 | ppl   288.69\n",
            "| epoch   1 |  1550/ 3195 batches | lr 5.00 | ms/batch  7.09 | loss  5.66 | ppl   288.32\n",
            "| epoch   1 |  1560/ 3195 batches | lr 5.00 | ms/batch  7.21 | loss  5.57 | ppl   263.75\n",
            "| epoch   1 |  1570/ 3195 batches | lr 5.00 | ms/batch  7.66 | loss  5.57 | ppl   262.05\n",
            "| epoch   1 |  1580/ 3195 batches | lr 5.00 | ms/batch  7.17 | loss  5.72 | ppl   305.72\n",
            "| epoch   1 |  1590/ 3195 batches | lr 5.00 | ms/batch  7.40 | loss  5.75 | ppl   314.28\n",
            "| epoch   1 |  1600/ 3195 batches | lr 5.00 | ms/batch  7.83 | loss  5.57 | ppl   262.40\n",
            "| epoch   1 |  1610/ 3195 batches | lr 5.00 | ms/batch  7.02 | loss  5.71 | ppl   301.45\n",
            "| epoch   1 |  1620/ 3195 batches | lr 5.00 | ms/batch  6.96 | loss  5.72 | ppl   305.27\n",
            "| epoch   1 |  1630/ 3195 batches | lr 5.00 | ms/batch  7.39 | loss  5.67 | ppl   290.76\n",
            "| epoch   1 |  1640/ 3195 batches | lr 5.00 | ms/batch  7.41 | loss  5.77 | ppl   322.07\n",
            "| epoch   1 |  1650/ 3195 batches | lr 5.00 | ms/batch  7.07 | loss  5.72 | ppl   305.66\n",
            "| epoch   1 |  1660/ 3195 batches | lr 5.00 | ms/batch  7.45 | loss  5.63 | ppl   277.54\n",
            "| epoch   1 |  1670/ 3195 batches | lr 5.00 | ms/batch  7.00 | loss  5.54 | ppl   255.14\n",
            "| epoch   1 |  1680/ 3195 batches | lr 5.00 | ms/batch  7.04 | loss  5.54 | ppl   254.34\n",
            "| epoch   1 |  1690/ 3195 batches | lr 5.00 | ms/batch  7.26 | loss  5.61 | ppl   274.25\n",
            "| epoch   1 |  1700/ 3195 batches | lr 5.00 | ms/batch  7.16 | loss  5.65 | ppl   282.93\n",
            "| epoch   1 |  1710/ 3195 batches | lr 5.00 | ms/batch  7.08 | loss  5.68 | ppl   292.26\n",
            "| epoch   1 |  1720/ 3195 batches | lr 5.00 | ms/batch  7.44 | loss  5.53 | ppl   251.39\n",
            "| epoch   1 |  1730/ 3195 batches | lr 5.00 | ms/batch  7.11 | loss  5.61 | ppl   272.06\n",
            "| epoch   1 |  1740/ 3195 batches | lr 5.00 | ms/batch  7.00 | loss  5.66 | ppl   286.54\n",
            "| epoch   1 |  1750/ 3195 batches | lr 5.00 | ms/batch  7.91 | loss  5.67 | ppl   289.22\n",
            "| epoch   1 |  1760/ 3195 batches | lr 5.00 | ms/batch  7.03 | loss  5.55 | ppl   257.04\n",
            "| epoch   1 |  1770/ 3195 batches | lr 5.00 | ms/batch  6.97 | loss  5.63 | ppl   277.52\n",
            "| epoch   1 |  1780/ 3195 batches | lr 5.00 | ms/batch  7.44 | loss  5.55 | ppl   257.71\n",
            "| epoch   1 |  1790/ 3195 batches | lr 5.00 | ms/batch  6.95 | loss  5.53 | ppl   251.22\n",
            "| epoch   1 |  1800/ 3195 batches | lr 5.00 | ms/batch  6.94 | loss  5.57 | ppl   261.41\n",
            "| epoch   1 |  1810/ 3195 batches | lr 5.00 | ms/batch  7.47 | loss  5.54 | ppl   254.10\n",
            "| epoch   1 |  1820/ 3195 batches | lr 5.00 | ms/batch  6.85 | loss  5.63 | ppl   279.17\n",
            "| epoch   1 |  1830/ 3195 batches | lr 5.00 | ms/batch  6.89 | loss  5.64 | ppl   281.16\n",
            "| epoch   1 |  1840/ 3195 batches | lr 5.00 | ms/batch  7.64 | loss  5.58 | ppl   263.77\n",
            "| epoch   1 |  1850/ 3195 batches | lr 5.00 | ms/batch  6.92 | loss  5.62 | ppl   276.01\n",
            "| epoch   1 |  1860/ 3195 batches | lr 5.00 | ms/batch  6.93 | loss  5.68 | ppl   292.21\n",
            "| epoch   1 |  1870/ 3195 batches | lr 5.00 | ms/batch  7.11 | loss  5.63 | ppl   277.51\n",
            "| epoch   1 |  1880/ 3195 batches | lr 5.00 | ms/batch  7.06 | loss  5.76 | ppl   318.84\n",
            "| epoch   1 |  1890/ 3195 batches | lr 5.00 | ms/batch  7.45 | loss  5.65 | ppl   283.02\n",
            "| epoch   1 |  1900/ 3195 batches | lr 5.00 | ms/batch  7.10 | loss  5.66 | ppl   288.16\n",
            "| epoch   1 |  1910/ 3195 batches | lr 5.00 | ms/batch  6.93 | loss  5.62 | ppl   277.19\n",
            "| epoch   1 |  1920/ 3195 batches | lr 5.00 | ms/batch  7.03 | loss  5.51 | ppl   246.50\n",
            "| epoch   1 |  1930/ 3195 batches | lr 5.00 | ms/batch  7.47 | loss  5.65 | ppl   285.42\n",
            "| epoch   1 |  1940/ 3195 batches | lr 5.00 | ms/batch  6.99 | loss  5.61 | ppl   274.36\n",
            "| epoch   1 |  1950/ 3195 batches | lr 5.00 | ms/batch  6.98 | loss  5.50 | ppl   243.60\n",
            "| epoch   1 |  1960/ 3195 batches | lr 5.00 | ms/batch  7.36 | loss  5.52 | ppl   250.18\n",
            "| epoch   1 |  1970/ 3195 batches | lr 5.00 | ms/batch  6.91 | loss  5.59 | ppl   268.41\n",
            "| epoch   1 |  1980/ 3195 batches | lr 5.00 | ms/batch  7.10 | loss  5.58 | ppl   266.34\n",
            "| epoch   1 |  1990/ 3195 batches | lr 5.00 | ms/batch  7.15 | loss  5.66 | ppl   287.78\n",
            "| epoch   1 |  2000/ 3195 batches | lr 5.00 | ms/batch  7.22 | loss  5.74 | ppl   310.27\n",
            "| epoch   1 |  2010/ 3195 batches | lr 5.00 | ms/batch  6.95 | loss  5.64 | ppl   280.92\n",
            "| epoch   1 |  2020/ 3195 batches | lr 5.00 | ms/batch  7.17 | loss  5.64 | ppl   280.59\n",
            "| epoch   1 |  2030/ 3195 batches | lr 5.00 | ms/batch  7.46 | loss  5.60 | ppl   269.85\n",
            "| epoch   1 |  2040/ 3195 batches | lr 5.00 | ms/batch  7.17 | loss  5.60 | ppl   270.13\n",
            "| epoch   1 |  2050/ 3195 batches | lr 5.00 | ms/batch  7.10 | loss  5.58 | ppl   265.85\n",
            "| epoch   1 |  2060/ 3195 batches | lr 5.00 | ms/batch  7.39 | loss  5.49 | ppl   243.46\n",
            "| epoch   1 |  2070/ 3195 batches | lr 5.00 | ms/batch  7.03 | loss  5.59 | ppl   266.94\n",
            "| epoch   1 |  2080/ 3195 batches | lr 5.00 | ms/batch  7.46 | loss  5.42 | ppl   226.82\n",
            "| epoch   1 |  2090/ 3195 batches | lr 5.00 | ms/batch  6.96 | loss  5.44 | ppl   230.38\n",
            "| epoch   1 |  2100/ 3195 batches | lr 5.00 | ms/batch  7.21 | loss  5.58 | ppl   263.77\n",
            "| epoch   1 |  2110/ 3195 batches | lr 5.00 | ms/batch  7.19 | loss  5.56 | ppl   260.58\n",
            "| epoch   1 |  2120/ 3195 batches | lr 5.00 | ms/batch  7.34 | loss  5.52 | ppl   248.70\n",
            "| epoch   1 |  2130/ 3195 batches | lr 5.00 | ms/batch  7.04 | loss  5.61 | ppl   273.79\n",
            "| epoch   1 |  2140/ 3195 batches | lr 5.00 | ms/batch  7.29 | loss  5.49 | ppl   243.09\n",
            "| epoch   1 |  2150/ 3195 batches | lr 5.00 | ms/batch  7.13 | loss  5.62 | ppl   275.04\n",
            "| epoch   1 |  2160/ 3195 batches | lr 5.00 | ms/batch  7.18 | loss  5.65 | ppl   284.85\n",
            "| epoch   1 |  2170/ 3195 batches | lr 5.00 | ms/batch  7.96 | loss  5.60 | ppl   271.50\n",
            "| epoch   1 |  2180/ 3195 batches | lr 5.00 | ms/batch  7.08 | loss  5.63 | ppl   279.36\n",
            "| epoch   1 |  2190/ 3195 batches | lr 5.00 | ms/batch  7.06 | loss  5.56 | ppl   260.07\n",
            "| epoch   1 |  2200/ 3195 batches | lr 5.00 | ms/batch  7.36 | loss  5.58 | ppl   264.37\n",
            "| epoch   1 |  2210/ 3195 batches | lr 5.00 | ms/batch  6.98 | loss  5.60 | ppl   270.80\n",
            "| epoch   1 |  2220/ 3195 batches | lr 5.00 | ms/batch  6.97 | loss  5.55 | ppl   257.77\n",
            "| epoch   1 |  2230/ 3195 batches | lr 5.00 | ms/batch  7.68 | loss  5.55 | ppl   256.12\n",
            "| epoch   1 |  2240/ 3195 batches | lr 5.00 | ms/batch  7.20 | loss  5.43 | ppl   228.23\n",
            "| epoch   1 |  2250/ 3195 batches | lr 5.00 | ms/batch  6.98 | loss  5.52 | ppl   248.82\n",
            "| epoch   1 |  2260/ 3195 batches | lr 5.00 | ms/batch  7.38 | loss  5.52 | ppl   249.67\n",
            "| epoch   1 |  2270/ 3195 batches | lr 5.00 | ms/batch  6.95 | loss  5.47 | ppl   236.85\n",
            "| epoch   1 |  2280/ 3195 batches | lr 5.00 | ms/batch  7.37 | loss  5.39 | ppl   218.96\n",
            "| epoch   1 |  2290/ 3195 batches | lr 5.00 | ms/batch  7.50 | loss  5.38 | ppl   216.82\n",
            "| epoch   1 |  2300/ 3195 batches | lr 5.00 | ms/batch  7.08 | loss  5.37 | ppl   215.77\n",
            "| epoch   1 |  2310/ 3195 batches | lr 5.00 | ms/batch  7.58 | loss  5.48 | ppl   239.64\n",
            "| epoch   1 |  2320/ 3195 batches | lr 5.00 | ms/batch  7.77 | loss  5.51 | ppl   247.12\n",
            "| epoch   1 |  2330/ 3195 batches | lr 5.00 | ms/batch  7.11 | loss  5.37 | ppl   215.50\n",
            "| epoch   1 |  2340/ 3195 batches | lr 5.00 | ms/batch  7.04 | loss  5.44 | ppl   229.49\n",
            "| epoch   1 |  2350/ 3195 batches | lr 5.00 | ms/batch  7.47 | loss  5.45 | ppl   233.80\n",
            "| epoch   1 |  2360/ 3195 batches | lr 5.00 | ms/batch  7.17 | loss  5.25 | ppl   190.69\n",
            "| epoch   1 |  2370/ 3195 batches | lr 5.00 | ms/batch  7.00 | loss  5.47 | ppl   236.88\n",
            "| epoch   1 |  2380/ 3195 batches | lr 5.00 | ms/batch  7.47 | loss  5.40 | ppl   220.36\n",
            "| epoch   1 |  2390/ 3195 batches | lr 5.00 | ms/batch  7.20 | loss  5.43 | ppl   227.50\n",
            "| epoch   1 |  2400/ 3195 batches | lr 5.00 | ms/batch  7.29 | loss  5.51 | ppl   246.30\n",
            "| epoch   1 |  2410/ 3195 batches | lr 5.00 | ms/batch  7.23 | loss  5.55 | ppl   256.74\n",
            "| epoch   1 |  2420/ 3195 batches | lr 5.00 | ms/batch  7.09 | loss  5.46 | ppl   236.26\n",
            "| epoch   1 |  2430/ 3195 batches | lr 5.00 | ms/batch  7.17 | loss  5.63 | ppl   277.71\n",
            "| epoch   1 |  2440/ 3195 batches | lr 5.00 | ms/batch  7.64 | loss  5.46 | ppl   235.29\n",
            "| epoch   1 |  2450/ 3195 batches | lr 5.00 | ms/batch  7.68 | loss  5.45 | ppl   233.61\n",
            "| epoch   1 |  2460/ 3195 batches | lr 5.00 | ms/batch  7.05 | loss  5.60 | ppl   270.13\n",
            "| epoch   1 |  2470/ 3195 batches | lr 5.00 | ms/batch  9.01 | loss  5.56 | ppl   258.54\n",
            "| epoch   1 |  2480/ 3195 batches | lr 5.00 | ms/batch  7.08 | loss  5.64 | ppl   281.25\n",
            "| epoch   1 |  2490/ 3195 batches | lr 5.00 | ms/batch  7.05 | loss  5.68 | ppl   293.50\n",
            "| epoch   1 |  2500/ 3195 batches | lr 5.00 | ms/batch  7.36 | loss  5.54 | ppl   253.97\n",
            "| epoch   1 |  2510/ 3195 batches | lr 5.00 | ms/batch  7.36 | loss  5.42 | ppl   224.85\n",
            "| epoch   1 |  2520/ 3195 batches | lr 5.00 | ms/batch  7.06 | loss  5.53 | ppl   253.30\n",
            "| epoch   1 |  2530/ 3195 batches | lr 5.00 | ms/batch  7.31 | loss  5.49 | ppl   243.38\n",
            "| epoch   1 |  2540/ 3195 batches | lr 5.00 | ms/batch  7.05 | loss  5.45 | ppl   233.67\n",
            "| epoch   1 |  2550/ 3195 batches | lr 5.00 | ms/batch  7.16 | loss  5.54 | ppl   253.87\n",
            "| epoch   1 |  2560/ 3195 batches | lr 5.00 | ms/batch  7.45 | loss  5.53 | ppl   252.86\n",
            "| epoch   1 |  2570/ 3195 batches | lr 5.00 | ms/batch  7.21 | loss  5.55 | ppl   257.63\n",
            "| epoch   1 |  2580/ 3195 batches | lr 5.00 | ms/batch  7.08 | loss  5.54 | ppl   255.08\n",
            "| epoch   1 |  2590/ 3195 batches | lr 5.00 | ms/batch  7.55 | loss  5.37 | ppl   214.70\n",
            "| epoch   1 |  2600/ 3195 batches | lr 5.00 | ms/batch  7.00 | loss  5.58 | ppl   266.36\n",
            "| epoch   1 |  2610/ 3195 batches | lr 5.00 | ms/batch  7.13 | loss  5.50 | ppl   245.58\n",
            "| epoch   1 |  2620/ 3195 batches | lr 5.00 | ms/batch  7.28 | loss  5.53 | ppl   251.62\n",
            "| epoch   1 |  2630/ 3195 batches | lr 5.00 | ms/batch  7.06 | loss  5.53 | ppl   252.12\n",
            "| epoch   1 |  2640/ 3195 batches | lr 5.00 | ms/batch  7.54 | loss  5.51 | ppl   246.37\n",
            "| epoch   1 |  2650/ 3195 batches | lr 5.00 | ms/batch  7.18 | loss  5.48 | ppl   240.02\n",
            "| epoch   1 |  2660/ 3195 batches | lr 5.00 | ms/batch  7.03 | loss  5.46 | ppl   233.95\n",
            "| epoch   1 |  2670/ 3195 batches | lr 5.00 | ms/batch  7.11 | loss  5.55 | ppl   256.79\n",
            "| epoch   1 |  2680/ 3195 batches | lr 5.00 | ms/batch  7.47 | loss  5.52 | ppl   249.95\n",
            "| epoch   1 |  2690/ 3195 batches | lr 5.00 | ms/batch  7.01 | loss  5.52 | ppl   248.98\n",
            "| epoch   1 |  2700/ 3195 batches | lr 5.00 | ms/batch  7.02 | loss  5.56 | ppl   260.40\n",
            "| epoch   1 |  2710/ 3195 batches | lr 5.00 | ms/batch  7.35 | loss  5.60 | ppl   269.88\n",
            "| epoch   1 |  2720/ 3195 batches | lr 5.00 | ms/batch  7.18 | loss  5.53 | ppl   252.79\n",
            "| epoch   1 |  2730/ 3195 batches | lr 5.00 | ms/batch  7.50 | loss  5.33 | ppl   206.98\n",
            "| epoch   1 |  2740/ 3195 batches | lr 5.00 | ms/batch  7.32 | loss  5.55 | ppl   257.86\n",
            "| epoch   1 |  2750/ 3195 batches | lr 5.00 | ms/batch  7.05 | loss  5.55 | ppl   258.32\n",
            "| epoch   1 |  2760/ 3195 batches | lr 5.00 | ms/batch  7.51 | loss  5.50 | ppl   243.64\n",
            "| epoch   1 |  2770/ 3195 batches | lr 5.00 | ms/batch  7.39 | loss  5.49 | ppl   242.83\n",
            "| epoch   1 |  2780/ 3195 batches | lr 5.00 | ms/batch  7.11 | loss  5.47 | ppl   238.03\n",
            "| epoch   1 |  2790/ 3195 batches | lr 5.00 | ms/batch  7.07 | loss  5.42 | ppl   225.26\n",
            "| epoch   1 |  2800/ 3195 batches | lr 5.00 | ms/batch  7.64 | loss  5.57 | ppl   262.42\n",
            "| epoch   1 |  2810/ 3195 batches | lr 5.00 | ms/batch  7.08 | loss  5.45 | ppl   232.57\n",
            "| epoch   1 |  2820/ 3195 batches | lr 5.00 | ms/batch  7.02 | loss  5.38 | ppl   217.80\n",
            "| epoch   1 |  2830/ 3195 batches | lr 5.00 | ms/batch  7.32 | loss  5.37 | ppl   213.94\n",
            "| epoch   1 |  2840/ 3195 batches | lr 5.00 | ms/batch  7.33 | loss  5.33 | ppl   206.39\n",
            "| epoch   1 |  2850/ 3195 batches | lr 5.00 | ms/batch  7.02 | loss  5.29 | ppl   198.18\n",
            "| epoch   1 |  2860/ 3195 batches | lr 5.00 | ms/batch  7.26 | loss  5.36 | ppl   211.71\n",
            "| epoch   1 |  2870/ 3195 batches | lr 5.00 | ms/batch  7.35 | loss  5.42 | ppl   226.80\n",
            "| epoch   1 |  2880/ 3195 batches | lr 5.00 | ms/batch  7.21 | loss  5.42 | ppl   225.07\n",
            "| epoch   1 |  2890/ 3195 batches | lr 5.00 | ms/batch  7.60 | loss  5.41 | ppl   222.92\n",
            "| epoch   1 |  2900/ 3195 batches | lr 5.00 | ms/batch  7.09 | loss  5.40 | ppl   220.48\n",
            "| epoch   1 |  2910/ 3195 batches | lr 5.00 | ms/batch  7.06 | loss  5.46 | ppl   235.74\n",
            "| epoch   1 |  2920/ 3195 batches | lr 5.00 | ms/batch  7.48 | loss  5.45 | ppl   231.88\n",
            "| epoch   1 |  2930/ 3195 batches | lr 5.00 | ms/batch  6.98 | loss  5.47 | ppl   237.60\n",
            "| epoch   1 |  2940/ 3195 batches | lr 5.00 | ms/batch  7.04 | loss  5.56 | ppl   260.41\n",
            "| epoch   1 |  2950/ 3195 batches | lr 5.00 | ms/batch  7.19 | loss  5.47 | ppl   237.26\n",
            "| epoch   1 |  2960/ 3195 batches | lr 5.00 | ms/batch  7.15 | loss  5.41 | ppl   222.70\n",
            "| epoch   1 |  2970/ 3195 batches | lr 5.00 | ms/batch 17.02 | loss  5.29 | ppl   199.23\n",
            "| epoch   1 |  2980/ 3195 batches | lr 5.00 | ms/batch 16.73 | loss  5.49 | ppl   242.17\n",
            "| epoch   1 |  2990/ 3195 batches | lr 5.00 | ms/batch  8.36 | loss  5.52 | ppl   248.53\n",
            "| epoch   1 |  3000/ 3195 batches | lr 5.00 | ms/batch  7.09 | loss  5.40 | ppl   222.00\n",
            "| epoch   1 |  3010/ 3195 batches | lr 5.00 | ms/batch  7.00 | loss  5.46 | ppl   234.03\n",
            "| epoch   1 |  3020/ 3195 batches | lr 5.00 | ms/batch  6.98 | loss  5.44 | ppl   230.60\n",
            "| epoch   1 |  3030/ 3195 batches | lr 5.00 | ms/batch  7.02 | loss  5.37 | ppl   214.71\n",
            "| epoch   1 |  3040/ 3195 batches | lr 5.00 | ms/batch  7.06 | loss  5.50 | ppl   244.26\n",
            "| epoch   1 |  3050/ 3195 batches | lr 5.00 | ms/batch  7.61 | loss  5.47 | ppl   236.74\n",
            "| epoch   1 |  3060/ 3195 batches | lr 5.00 | ms/batch 11.47 | loss  5.41 | ppl   223.15\n",
            "| epoch   1 |  3070/ 3195 batches | lr 5.00 | ms/batch 19.94 | loss  5.53 | ppl   252.33\n",
            "| epoch   1 |  3080/ 3195 batches | lr 5.00 | ms/batch  7.87 | loss  5.42 | ppl   225.11\n",
            "| epoch   1 |  3090/ 3195 batches | lr 5.00 | ms/batch  6.91 | loss  5.30 | ppl   201.22\n",
            "| epoch   1 |  3100/ 3195 batches | lr 5.00 | ms/batch  7.61 | loss  5.61 | ppl   272.76\n",
            "| epoch   1 |  3110/ 3195 batches | lr 5.00 | ms/batch  7.07 | loss  5.27 | ppl   194.96\n",
            "| epoch   1 |  3120/ 3195 batches | lr 5.00 | ms/batch  7.00 | loss  5.42 | ppl   226.45\n",
            "| epoch   1 |  3130/ 3195 batches | lr 5.00 | ms/batch  7.60 | loss  5.36 | ppl   212.91\n",
            "| epoch   1 |  3140/ 3195 batches | lr 5.00 | ms/batch  7.07 | loss  5.45 | ppl   232.73\n",
            "| epoch   1 |  3150/ 3195 batches | lr 5.00 | ms/batch  7.20 | loss  5.48 | ppl   239.73\n",
            "| epoch   1 |  3160/ 3195 batches | lr 5.00 | ms/batch  7.23 | loss  5.30 | ppl   200.98\n",
            "| epoch   1 |  3170/ 3195 batches | lr 5.00 | ms/batch  7.06 | loss  5.43 | ppl   228.05\n",
            "| epoch   1 |  3180/ 3195 batches | lr 5.00 | ms/batch  7.00 | loss  5.32 | ppl   203.43\n",
            "| epoch   1 |  3190/ 3195 batches | lr 5.00 | ms/batch  7.44 | loss  5.34 | ppl   207.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 25.21s | valid loss  5.20 | valid ppl   181.70\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "epochs = 1 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print(\"starting\")\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgNfblVjucsp"
      },
      "source": [
        "#### 2.3.2 Why do we need to use `torch.nn.utils.clip_grad_norm_` in training?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB9R4RhvTShb"
      },
      "source": [
        "Set a threshold for gradient clipping, and if the gradient exceeds this threshold when updating the gradient, it will be limited to this range to prevent the gradient from exploding."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "CS523 Homework 5 (Zhenghang Yin)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}